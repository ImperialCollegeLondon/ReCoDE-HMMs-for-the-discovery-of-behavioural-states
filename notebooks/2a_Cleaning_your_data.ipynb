{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the HMM to the sleep dataset\n",
    "======"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the dataset we'll be working with for this tutorial, a Drosophila movement dataset. Data can come in all formats, but a CSV file is one of the most common due to its simplicity and integration with spreadsheets. The data we will be using for this tutorial is real, raw data from the Gilestro lab, where we track and record the movement of fruit flies using machine vision. The tracking is able to discern small movements in the fly that can robustly record the flies several times per second, giving a multitude of variables to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the Pandas package to import and store our data in the notebooks. Pandas is a widely used tool in data science; it is built on top of Numpy, which we briefly used previously. At the core of Pandas is the DataFrame, a table format you will all be familiar with from spreadsheets. Pandas gives you many tools to manipulate the data before you feed it into any analysis or machine learning tool. As with Numpy, everything used here will be explained as we use it, but if you'd like to read more about how to use Pandas, there is a quick tutorial on their website.  -> [here](https://pandas.pydata.org/docs/user_guide/10min.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to import pandas and numpy\n",
    "# like numpy it is often imported in a shorthand \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# this pandas setting is to suppress warnings in a later function\n",
    "pd.set_option(\"future.no_silent_downcasting\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas can read many different formats; see [here](https://pandas.pydata.org/docs/reference/io.html) for a detailed list of all file types that can be read and saved to. One of the most common in biology are spreadsheets, or csv files. The training data for this tutorial is saved as a zipped csv file, saved in the data folder called 'training_data.zip'.\n",
    "\n",
    "\n",
    "Copy the path of that file (see below for the exact file structure) and save it as the variable 'path' and as a string. Then load into the notebook using the function [pd.read_csv()](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html#pandas.read_csv).\n",
    "\n",
    "```log\n",
    ".\n",
    "|\n",
    "├── data\n",
    "|   ├── example_hmm.pkl\n",
    "|   ├── training_data_metadata.csv\n",
    "|   └── training_data.zip <----\n",
    "├── docs\n",
    "├── notebooks\n",
    "|   ├── 1_Understanding_HMMs.ipynb\n",
    "|   ├── 2a_Cleaning_your_data.ipynb\n",
    "|   └── ...\n",
    "└── src\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using windows you'll need to put an r in front of the string as python doesn't like backslashes\n",
    "# .e.g r'C:\\Users\\USER\\Documents\\HMM_tutorial\\data\\training_data.zip'\n",
    "\n",
    "path = ''\n",
    "\n",
    "path = '/home/lab/Desktop/ReCoDE-HMMs-for-the-discovery-of-behavioural-states/data/training_data.zip'\n",
    "# its common practice to save dataframes as df\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data Structure\n",
    "================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is 'id' which contains a unique ID per fly and will allow us to filter and apply methods to just one fly at a time. The next most important variable is 't' or time. As we are working with time series data, we must ensure this is structured properly, i.e., in sequential order and at regular intervals (the later we will go over). The rest are various variables per timestamp; for this tutorial, we'll only be interested in 'moving', 'micro', and 'walk'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for missing data\n",
    "======"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most real datasets will not be perfectly populated, with tracking dropping out over the course of an experiment. In a dataframe or an array where there is data missing at a timepoint or index, this will be represented by a NaN value, which lets methods and functions know there is no data rather than a zero value. However, often analysing packages will throw an error if you feed it NaN values, so it's good practice to check for them first and either remove them or replace them with an approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets filter our dataframe for nan values\n",
    "# With pandas you can filter the dataframe by the columns\n",
    "# To filter or slice the dataframe put some square brackets after the dataframe and inside call the column slice \n",
    "# For finding NaN values we have to call a method, for other regular filtering you just use =, <, > and so on\n",
    "\n",
    "df[df['x'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To break down whats happening we can just call whats inside the brackets, you can see that it is an array (or series in pandas terms) with False or True per row.\n",
    "# This array then dictates what rows get returned from the whole dataframe, i.e. only the ones that fullfil the argument and are True\n",
    "\n",
    "df['x'].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, we are not just looking at one column. \n",
    "# Luckily with pandas you can filter by multiple columns, all you need to do is put each filter argument in round brackets and then separate them by an & (\"and\") or | (\"or\") logical operator\n",
    "# By calling OR here we get all the examples where X or Y are NaNs\n",
    "\n",
    "df[(df['x'].isnull()) | (df['y'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to remove those rows containing NaN values as they aren't providing any information\n",
    "# This time we'll want to call the & operator as we only want rows where both X and Y are not NaNs\n",
    "# When filtering for NaNs above we're selecting for them, adding the ~ opertor tells the filter to look for the opposite, so when NaN is True it now becomes False\n",
    "# If taking a slice of a dataframe its good practice to make it a copy, otherwise it will throw up warnings\n",
    "df_filtered = df[~(df['x'].isnull()) & ~ (df['y'].isnull())].copy(deep = True)\n",
    "\n",
    "# the new DataFrame now won't have any rows where 'x' and 'y' have NaN values\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task:\n",
    "As stated before for this tutorial, we will be focusing on the variables 'moving', 'micro', 'walk'. Now you know how to filter out NaN values apply this to only these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete \n",
    "\n",
    "df = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Task:\n",
    "1) If you're new to Pandas (or just want some practice), have a play around with other types of filtering (such as df[df['mean_velocity'] > 5]). It's a quick and easy way to filter your data, and if you're doing the same thing repeatedly, you can create a function to do it instantly.\n",
    "\n",
    "\n",
    "2) Rather than filtering out the NaN values, you can replace them with something else. We could know that tracking drops out when the flies are still for a long time, so we could resonably replace all of 'moving', 'micro', and 'walk' with False.\n",
    "This can be done with the .fillna method, see here for how to do it -> [fillna](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binning the data to a larger time step\n",
    "======"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important with hidden Markov models that any timeseries dataset is complete with no skips due to missing data, as the model will assume the array you're feeding it all has the same time step. One way to do this is to increase the timestep; currently, the dataset has a row for every 30 seconds. However, we know from filtering out the NaN values that we won't have them all. So to achieve this, we will increase the time step to 60. As long as there is at least 1 row out of 2 for the 60, we'll have a perfectly populated dataset.\n",
    "\n",
    "\n",
    "Additionally, doing so will decrease the size of the data, meaning the model will train more quickly. It's always worth trying the model with a few different timesteps to see how this affects the output, and then you can pick the one you think is the most representative and quickest to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll go through it step by step, before wrapping it all in a function\n",
    "# First we'll need the function floor which rounds down numbers\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we'll create a new column with the new time step\n",
    "# lambda functions are an easy way to apply a function per row with a specific column\n",
    "# We then divide the time by our new time and round down. The end result is multiplied by the new time step giving the minimum time as divisable by the time step given\n",
    "df['bin_t'] = df['t'].map(lambda t: 60 * floor(t / 60)) # the t represenst each row value for the column 't'\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see in the column 'bin_t' that rows next to each other now share a time step. Now we have that we'll want to pivot or group by this column so all that have the same time stamp are collected together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pandas groupby method does this, all you need to do is call the method with the column you want to pivot by in the brackets\n",
    "# Then you can tell it what aggregating function you want to call on the columns of interest\n",
    "df_grouped = df.groupby('bin_t').agg(**{\n",
    "            'x' : ('x', 'mean'), # before the brackets is the name of the new column, we'll keep it the same\n",
    "            'y' : ('y', 'mean')  # within the brackets is the column you want to use and the function to apply to it. You have 'mean', 'median', 'max'... ect built in, but you can also use your own functions\n",
    "})\n",
    "\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of you may have noticed that doing it this way will aggregate our whole dataset and lose information per fly. To keep this information, we can call it a groupby with two levels: the first will be the higher level that the data is grouped by first, and the second will be the one that the functions will be applied to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do exactly the same, but instead of just 'bin_t' we have a list with 'id' first\n",
    "# Calling it this way on a lot of rows can take a few minutes or more depending on your computer, so don't worry if it takes a while\n",
    "df_grouped = df.groupby(['id', 'bin_t']).agg(**{\n",
    "            'x' : ('x', 'mean'),\n",
    "            'y' : ('y', 'mean')\n",
    "})\n",
    "# We need to reset the index as it will have both 'id' and 'bin_t' as the index\n",
    "df_grouped.reset_index(inplace = True)\n",
    "# We'll also rename the column 'bin_t' back to 't' for clarity\n",
    "df_grouped.rename(columns = {'bin_t' : 't'}, inplace = True)\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task:\n",
    "The Same as before, recreate the steps above, but for the columns 'moving', 'micro', 'walk'. Instead of mean, use max, as we care about the most dominant behaviour in that time window, and it will also keep our results as either True or False which are discrete categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete \n",
    "\n",
    "df = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling in the gaps\n",
    "========"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method to fill in the gaps in the data is interpolation. This is where you determine a value at any given timepoint, given the rest of the dataset. If you have just a few points missing, the interpolation results can be quite accurate. Here we'll run through the steps to interpolate your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we can check if we have all the data points for each fly\n",
    "# We'll use this method to check\n",
    "\n",
    "def check_num_points(data, timestep=60):\n",
    "    array_time = max(data['t']) - min(data['t'])\n",
    "\n",
    "    if (array_time / timestep) + 1 == len(data):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# We need to call the function on each fly individually\n",
    "# To do this you cal a groupby with 'id' as the argument\n",
    "df_check = df.groupby('id').apply(check_num_points, include_groups=False) # set include_groups to false when you don't want the grouping column in the analysis\n",
    "\n",
    "# This gives us a pandas series of True and False for each fly\n",
    "df_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can count all the True and Falses with the method .value_counts()\n",
    "df_check.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that nearly 50% of our flies are missing some points, so it's best we move ahead with interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Task:\n",
    "Rather than just returning True or False, you can create a function that returns the percentage of points you have for the amount needed. You can then combine this to filter out the flies that have less than 75% of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code space for extra task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like when checking for points, we'll need to create a function that we can call to apply the interpolation per fly, as we want it to only use each fly's data. But we'll walk through the steps before creating it. As the data is discrete, we'll be using forward filling interpolation, which propagates the last valid observation to the next, see [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ffill.html) for more information.\n",
    "\n",
    "\n",
    "If we were working on continuous data, we could use linear interpolation, which we'll briefly demonstrate at the end with np.interp, a one-dimensional linear interpolator, see [here](https://numpy.org/devdocs/reference/generated/numpy.interp.html#numpy.interp) for the documentation from numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we'll work with a sub-sample of the main DataFrame so we can check things are working before creating the function\n",
    "\n",
    "# Task:\n",
    "# With the 'id' of '2016-04-04_17-38-06_019aee|20', create a sub DataFrame with just this data\n",
    "\n",
    "small_df = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll want to create a time series that contains all the points we want\n",
    "# For this we can use np.arange which creates an array from a given start point to an end point, with regular steps\n",
    "# You need to add on you time step to the end as it will only give it to the one step below otherwise\n",
    "ts_seq = np.arange(min(small_df['t']), max(small_df['t']) + 60, 60)\n",
    "\n",
    "# You can see it's an array that increase by 60 at each step by checking the difference per point\n",
    "np.all(np.diff(ts_seq) == 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we'll need to merge this back with the data to create new rows with NaN values which we'll replace\n",
    "# To do this we make a pandas series, which is like singular column dataframe, named 't'\n",
    "# With both the small dataframe and the series containing 't' we can merge the two together using this column as the key\n",
    "ts_seq = pd.Series(ts_seq, name = 't')\n",
    "small_df = small_df.merge(ts_seq, on = 't', how = 'right') # The merge is down to the right as we want the final result to be the length of the new sequence\n",
    "\n",
    "# Checking for NaN values we can see the new time points are all there\n",
    "small_df[small_df['moving'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now all we need to call is ffill\n",
    "small_df.ffill(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The NaNs have bbeen filled\n",
    "small_df[small_df['moving'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make a function that will complete this for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the missing parts with what we've done above\n",
    "\n",
    "def fill_interpolate(data, timestep = 60):\n",
    "\n",
    "    ts_seq = \n",
    "    ts_seq = \n",
    "\n",
    "    new_df = \n",
    "    new_df.ffill(inplace=True)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now call a groupby method, applying the interpolate function\n",
    "df = df.groupby('id').apply(fill_interpolate, include_groups=False)\n",
    "df.reset_index(level = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use the check function to see if its worked\n",
    "df_check = df.groupby('id').apply(check_num_points, include_groups=False)\n",
    "df_check.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear interpolation\n",
    "For continuous data like the X and Y coordinates, we can use linear interpolation that fills in the data given where it would be placed on a fitted linear line of true data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll load in the original dataset to get some continuous data again.\n",
    "interp_df  = pd.read_csv(path)\n",
    "\n",
    "# We'll check to see if any are missing datapoints\n",
    "df_check = interp_df.groupby('id').apply(check_num_points, include_groups=False)\n",
    "df_check.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're all missing points, so we'll use the same specimen as last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_interp = interp_df[interp_df['id'] == '2016-04-04_17-38-06_019aee|20'].copy(deep=True) \n",
    "small_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like before we'll make a new time series of the length and intervals we want\n",
    "ts_seq = np.arange(min(small_interp['t']), max(small_interp['t']) + 60, 60)\n",
    "\n",
    "# Call np.interp with the new time series first, the old second, and the corresponding data third\n",
    "new_seq = np.interp(ts_seq, small_interp['t'].to_numpy(), small_interp['x'].to_numpy())\n",
    "new_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra task:\n",
    "- Can you make a function that will use np.interp on the whole interp_df dataset per fly for variables 'x' and 'y'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code space for extra task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do this for the rest of the columns quickly with a for loop\n",
    "# for loops are useful for when you need to do the same thing over and over, with a few things changed\n",
    "for i in ['moving', 'micro', 'walk']:\n",
    "    small_df[i] = np.where(small_df[i] == True, 1, 0)\n",
    "\n",
    "# The columns are now nicely binary\n",
    "small_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we don't know if each flies data has the a good amount of data points for it\n",
    "# Flies with a low amount could indicate they died early or the tracking stopped working\n",
    "\n",
    "len_check = df.groupby('id').agg(**{\n",
    "    'length' : ('t', len)\n",
    "})\n",
    "len_check['length'].value_counts()\n",
    "\n",
    "# You can see most flies have over 9000 data points, but 2 have only 200 odd, we'll want to remove them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can find the length of each subset dataframe\n",
    "# get the ids of those > 300 data points\n",
    "# use this list to filter the whole dataset\n",
    "\n",
    "len_df = df.groupby('id').agg(**{\n",
    "    'len' : ('t', len)\n",
    "})\n",
    "filt_len = len_df[len_df['len'] < 300]\n",
    "filt_list = filt_len.index.tolist()\n",
    "\n",
    "df = df[~df['id'].isin(filt_list)]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coding our categories\n",
    "======="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HMM we'll be using is categorical, which means if we want to use all the information for the 3 columns, we must create a new column that has numbers that represent each variable when they are true. Hmmlearn takes each observable as a number, with the first being 0, the next being 1, and so on. Here we have 3 observables: not moving, micro-moving, and walking, so we would like them to be 0, 1, and 2, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do this we'll use np.where\n",
    "# np.where will search a row given a logic question and create a new one with answers depending on if the question comes back True or False\n",
    "\n",
    "# At first we'll look for all rows where the flies aren't moving, if True we label it 0, if not we give it a NaN value for now\n",
    "df['hmm'] = np.where(df['moving'] == 0, 0, np.nan)\n",
    "\n",
    "# Next we'll look at micro, a fly cannot be both micro moving and walking, they are distinct. So we can just select for True cases.\n",
    "# We make the False argument what the column was previously to keep the old category\n",
    "df['hmm'] = np.where(df['micro'] == 1, 1, df['hmm'])\n",
    "\n",
    "# Now we'll finish with walk, can you complete it?\n",
    "# df['hmm'] = np.where()\n",
    "df['hmm'] = np.where(df['walk'] == 1, 2, df['hmm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, we need to set the new column to only integers. Pandas has a habit of making data points floats, i.e., a whole number with a decimal point. This will cause problems when training the data, as the model for categorical hmms wants the input to only be integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can set columns types with the .astype() method\n",
    "df = df.astype({'hmm' : 'int'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we'll save our cleaned dataframe as a pickle file \n",
    "# Pickles are a popular format to save multiple variable formts\n",
    "# Pandas has a built-in function to save the file\n",
    "df.to_pickle('YOUR_PATH/data/cleaned_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Extra Tasks**\n",
    "\n",
    "\n",
    "## 1. Split the data by Male and Female into separate dataframes.\n",
    "- In the same folder (data) as the training data, there is a csv file containing the metadata for the experiment. This includes a column called \"sex,\" which denotes their sex by \"M\" for male and \"F\" for female, along with a column with their ID. Use both of these to filter and split the data into two training datasets.\n",
    "\n",
    "\n",
    "## 2. Convert a continuous float column to a discrete categorical column\n",
    "- Convert one of the continuous data columns [\"x\", \"y\", \"phi\", \"w\", \"h\"] into discrete categories and use them to train a HMM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
