{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring Models\n",
    "======"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last notebook, we learned how to select the best model given a set model architecture, but often you may have multiple versions of the architecture in mind that you would like to test. A common method is to use probabilistic statistical measures that attempt to quantify both the model performance on the training dataset and the complexity of the model. The scores often used are [Akaike](https://builtin.com/data-science/what-is-aic) and [Bayseian](https://medium.com/@analyttica/what-is-bayesian-information-criterion-bic-b3396a894be6) Information Criterion (AIC & BIC respectively). Both evaluate the model's fit on the training data, adding penalties for more complex models as these tend to overfit to the dataset. This means the scores will reflect the model that best generalises to the dataset.\n",
    "\n",
    "\n",
    "With criterion values, the lower the score, the better, and it is relative, which means it can only be compared with other models trained on the same dataset and in the same way. Both AIC and BIC evaluate in very similar ways with minor differences in their formulas, so the results should often be very similar.\n",
    "\n",
    "\n",
    "It is important to understand the limitations of probabilistic scores for models when viewing the results. Both AIC and BIC will by design prioritise the simplest model that best fits the dataset, it will have no knowledge of the uncertainty of the model or any biological relevance. It is therefore up to you to decide, given your prior knowledge of the system, which of the best performing models to pick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we will load in the whole dataset and train a fully open HMM (as in everything can transition into each other and all states can emit all observables) for the sake of ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll load in the pacakges we need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "from hmmlearn.hmm import CategoricalHMM\n",
    "\n",
    "# Load in your cleaned dataset\n",
    "df = pd.read_pickle('/USERS_PATH/ReCoDE-HMMs-for-the-discovery-of-behavioural-states/admin/cleaned_data.pkl')\n",
    "\n",
    "# List the observables\n",
    "observables = ['immobile', 'micro', 'walking']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform all the data to the right shape\n",
    "\n",
    "ar_data = df.groupby('id')['hmm'].apply(np.array)\n",
    "ar_data = np.array(ar_data)\n",
    "\n",
    "len_seq_all = [len(ar) for ar in ar_data]\n",
    "\n",
    "seq_all = np.concatenate(ar_data, axis = 0) \n",
    "seq_all = seq_all.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = ar_data[0]\n",
    "seq = seq.reshape(-1, 1) # It also needs to be reshped for decoding\n",
    "\n",
    "# Call the .decode() method with the sequence inside the brackets\n",
    "# The method returns two parts, the log liklihood for the sequence and the decoded sequence\n",
    "log_prob, decoded_array = model.decode(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All hmmlearn hidden markov models have a built in method that will give you AIC, BIC scores, as well as the .score() method we've used previously that gives the log likelihood. We'll run through briefly how to get these scores before creating models with varying numbers of hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load in the 2 state and 4 state model you trained previously into list called models\n",
    "models = []\n",
    "# replace the paths below with the ones in your repo\n",
    "models.append(pickle.load(open('.../ReCoDE-HMMs-for-the-discovery-of-behavioural-states/admin/2_state_model.pkl', \"rb\")))\n",
    "models.append(pickle.load(open('.../ReCoDE-HMMs-for-the-discovery-of-behavioural-states/admin/4_state_model.pkl', \"rb\")))\n",
    "\n",
    "# We'll create some empty lists to append the scores\n",
    "aic = []\n",
    "bic = []\n",
    "lls = []\n",
    "\n",
    "for hmm in models:\n",
    "    aic.append(hmm.aic(seq_all, len_seq_all)) # get the AIC score with .aic()\n",
    "    bic.append(hmm.bic(seq_all, len_seq_all)) #  get the BIC score with .bic()\n",
    "    lls.append(hmm.score(seq_all, len_seq_all))# get the logliklihood will .score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use Matplotlib to plot the data. If you've used Python before, you've probably seen or used [Matplotlib](https://matplotlib.org/) before, if not, it's a library for visualising data in Python. Click the embedded link for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the way to load matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Labels for the x-axis\n",
    "model_names = ['2 states', '4 states']\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot AIC and BIC on the first y-axis\n",
    "ln1 = ax.plot(model_names, aic, label=\"AIC\", color=\"blue\", marker=\"s\")\n",
    "ln2 = ax.plot(model_names, bic, label=\"BIC\", color=\"green\", marker=\"D\")\n",
    "# Create a second y-axis for logliklihood as its scores differently\n",
    "ax2 = ax.twinx()\n",
    "ln3 = ax2.plot(model_names, lls, label=\"LL\", color=\"orange\", marker=\"o\")\n",
    "\n",
    "# Joins the legends and sets the labels\n",
    "ax.legend(handles=ax.lines + ax2.lines)\n",
    "ax.set_title(\"Using AIC/BIC for Model Selection\")\n",
    "ax.set_ylabel(\"Criterion Value (lower is better)\")\n",
    "ax2.set_ylabel(\"LL (higher is better)\")\n",
    "ax.set_xlabel(\"HMM type\")\n",
    "fig.tight_layout()\n",
    "\n",
    "# Pring the plot to screen\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower the AIC and BIC, the better, and the higher the likelihood, the better. From this, we can see that despite the additional complexity of the four-state model, it performs better on the dataset in all scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task \n",
    "====\n",
    "\n",
    "Create a loop below that evaluates the models, with varying amounts of hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finsih the loop below using what you created in the previous notebook\n",
    "# hint for storing the best model\n",
    "\n",
    "aic = []\n",
    "bic = []\n",
    "lls = []\n",
    "\n",
    "# Create models of size 2, 4, 6, 8\n",
    "n_states = [2, 4, 6, 8]\n",
    "\n",
    "for n in n_states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to plot the scores here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head to notebook_answers for an example of what the graph should look like, as well as some commentary on how to interpret it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra task\n",
    "=====\n",
    "Try training models with varying numbers of hidden states that are true to the biology (i.e., the sleep states only emit as immobile and sleep stages are sequential).\n",
    "\n",
    "\n",
    "Compare the scores for each model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
