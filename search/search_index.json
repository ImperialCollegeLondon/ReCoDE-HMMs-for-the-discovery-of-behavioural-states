{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#_1","title":"Home","text":""},{"location":"#hidden-markov-models-for-the-discovery-of-behavioural-states","title":"Hidden Markov Models for the discovery of behavioural states","text":""},{"location":"#description","title":"Description","text":"<p>This is an exemplar project to help you understand the concepts behind the Hidden Markov Model (HMM), how to implement one with the python package hmmlearn, and finally how to explore the decoded data.</p> <p>HMMs are widely used in multiple fields, including biology, natural language processing, and finance as a predictor of future states in a sequence. However, here we will be utilising the hidden model states to create a hypothesied internal behavioural architecture.</p> <p>The tutorial will also run briefly through how to clean and augment a real world dataset using numpy and pandas, so that it's ready for training with hmmlearn.</p> <p>The information in tutorial was primarily designed around the user completing reading along and completing a jupyter notebook in python. But can followed loosely from just these pages. If reading along ignore any sections asking to complete any code (or complete it in your mind).</p> <p>This is all a part of the ReCoDE Project at Imperial College London</p>"},{"location":"#learning-outcomes","title":"Learning Outcomes","text":"<p>Only a basic understanding of python is needed prior to beginning, with the tutorials walking you through the use of numpy and pandas to curate data for use with the hmmlearn package.</p> <ul> <li> <ol> <li>Understanding the core concepts of HMMs</li> </ol> </li> <li> <ol> <li>Curating data and training/validating your own HMM</li> </ol> </li> <li> <ol> <li>Visualising and understanding your decoded data</li> </ol> </li> </ul>"},{"location":"#requirements","title":"Requirements","text":""},{"location":"#academic","title":"Academic","text":"<p>A basic knowledge of python is needed.</p> <p>The tutorial will be based in numpy and pandas, two data science packages for working with and manipulating data.</p> <p>No prior knowledge of HMMs is needed, nor deep understanding of mathmatical modelling. However, if you do want to read more about HMMs, I found this resource very useful when starting out: Hidden Markov Models - Speech and Language Processing</p>"},{"location":"#system","title":"System","text":"Program Version Python &gt;= 3.11.0 Git &gt;= 2.43.0"},{"location":"#packages","title":"Packages","text":"Package Version numpy &gt;= 1.26.4 pandas &gt;= 2.2.0 hmmlearn &gt;= 0.3.0 Matplotlib &gt;= 3.8.3 seaborn &gt;= 0.13.2 tabulate &gt;= 0.9.0 jupyter &gt;= 1.0.0"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#workflow","title":"Workflow","text":"<p>The tutorial will be taught through sequential jupyter notebooks which you will clone to your local computer. The code will be mainly written out and executed from within the notebooks so you will get a feel for the full workflow to generate and test HMMs. A few parts of the code that help the code run or tidy up the plots will be imported from elsewhere in the project.</p> <p>In the folder src there is a jupyter notebook called notebook_answers.ipynb. This notebook contains the answers to parts of the notebook where you need to write your own code.</p> <p>Once you've cloned the repo and installed the dependencies, open the first notebook as highlighted by the arrow in the structure below.</p>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>.\n|\n\u251c\u2500\u2500 data\n|   \u251c\u2500\u2500 example_hmm.pkl\n|   \u251c\u2500\u2500 training_data_metadata.csv\n|   \u2514\u2500\u2500 training_data.zip\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 notebooks\n|   \u251c\u2500\u2500 1_Understanding_HMMs.ipynb &lt;---\n|   \u251c\u2500\u2500 2a_Cleaning_your_data.ipynb\n|   \u251c\u2500\u2500 2b_Training.ipynb\n|   \u251c\u2500\u2500 2c_Validating.ipynb\n|   \u2514\u2500\u2500 3_Visualising_the_results.ipynb\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 hmm_plot_functions.py\n    \u251c\u2500\u2500 misc.py\n    \u2514\u2500\u2500 notebook_answers.ipynb\n</code></pre>"},{"location":"#workstation","title":"Workstation","text":"<p>You'll need something to run and edit the code in the notebooks as we go along. This tutorial was created in Visual Studio Code, but you can use whatever code editor you like.</p>"},{"location":"#cloning-the-repository","title":"Cloning the repository","text":"<p>If you don't have it already, install git to your machine, see here for details on all OS's.</p> <p>Once installed, run the following command in the terminal after moving to the location where you want it saved. We'll then 'cd' into the created folder.</p> <pre><code>git clone https://github.com/ImperialCollegeLondon/ReCoDE-HMMs-for-the-discovery-of-behavioural-states.git HMM_tutorial\ncd HMM_tutorial\n</code></pre>"},{"location":"#setting-up-your-environment","title":"Setting up your environment","text":"<pre><code>python -m venv .venv\nsource .venv/bin/activate # with Powershell on Windows: `.venv\\Scripts\\Activate.ps1`\n</code></pre>"},{"location":"#install-requirements","title":"Install requirements","text":"<pre><code>pip install -r requirements.txt\n</code></pre> <p>You're now ready to jump into the first notebook. Open up the notebook 1_Understanding_HMMs.ipynb.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>"},{"location":"1_Understanding_HMMs/","title":"Why Hidden Markov Models?","text":"<p>Often in biology there are complex systems, such as the internal apparatus of an organism or an ecosystem, where we have substantial knowledge but not enough to fully describe or accurately measure the changes within. We therefore want to reduce the system to easily understandable states that encapsulate our perceived understanding of the underlying biology.</p> <p>This is where the Hidden Markov models (HMM) can come in useful. Briefly, a HMM is a mathematical framework that describes a two-tiered system, where there is an observable system (the one we measure) whose changes over time can be inferred to\u00a0 represent the changes in a hidden system, one that is often more complex than the observable. We'll go into a bit more depth later about the underling principles, but a key aspect is knowing when to apply a HMM.\u00a0</p> <p>Below is a simple checklist you can run through when considering a HMM for your system:</p> <ol> <li><p>Is your observable data sequential, i.e., a time series dataset or a linear structure such as protein sequences</p> </li> <li><p>Can the underlying system (hidden states) be reasonably represented as distinct, discrete states</p> </li> <li><p>Is it reasonable to think these hidden states are associated with our observable data</p> </li> <li><p>Can I interpret and test the outputs to validate the model</p> </li> <li><p>Can the system be modelled under the Markovian assumption (don't worry, we'll get to that, but it's very simple)</p> </li> </ol> <p>We'll use this as a point to introduce the system we'll be modelling throughout this tutorial: sleep in Drosophila melanogaster. Let's go through some background on the topic to help us answer the checklist above. Sleep has been studied in fruit flies since 2000, after sleep-like states were observed when the flies were immobile, causing a boom in studies looking to utilise all the genetic tools in fruit flies to gain a better understanding of the general theory of sleep. The flies can be housed in tubes, and their activity can be measured for many days with external inputs to change their environment.</p> <p>For a long time, sleep in flies was mostly considered a uniform, monophasic event, something at odds with our knowledge of mammalian, bird, and reptile sleep, which has several phases. However, the gold standard for determining sleep states, EEGs, cannot be applied to fruit flies due to the differing morphology of their brains. New studies using alternative methods have shown changes in the whole brain activity throughout sleep bouts that would indicate changing sleep states. But the methods are very invasive and difficult to setup, leading to the question: can we predict these internal changes from more easily accessible data?</p> <p>The data we'll be working with is movement data tracked by an ethoscope, a device that can track ~20 flies at once through machine vision. You can read more about them in this publication.</p> <p> </p> <p>Above, in the bottom-most picture, you can see the flies enclosed in red. The Ethoscope will track the flies, recording their position on a microscale. The x,y coordinates will be recorded in a database with a timestamp to give us a multivariable time series. We'll look at and get to grips with this dataset in the next notebook. But first, let's apply the system and data to the above check list.</p> <p>We'll apply this to the checklist:</p> <ol> <li><p>Is your observable data sequential, i.e., a time series dataset or a linear structure such as protein sequences Yes - the data is a time series tracking each individual fly across the day.</p> </li> <li><p>Can the underlying system (hidden states) be reasonably represented as distinct, discrete states Yes - Our understanding of mammals (and birds and reptiles) is of distinct sleep states, so we can apply that to flies.</p> </li> <li><p>Is it reasonable to think these hidden states are associated with our observable data Quite likely - most animals are immobile when sleeping with pre-sleep rituals that are driven by an internal change</p> </li> <li><p>Can I interpret and test the outputs to validate the model Yes - Sleep is often measured by the change in arousability, so we can test the predicted states against this.</p> </li> <li><p>Can the system be modelled under the Markovian assumption (don't worry we'll get to that, but it's very simple) We'll answer this one below.</p> </li> </ol> <p>Now that we have passed all the checks, let's have a deeper look at Markov models to understand how to use them.</p> <p>HMMs are probabilistic models that work as a sequence of labelling problems, where the labelling problem is how a chain of observable events is determined by internal factors that can't be directly observed.</p> <p>Both parts of a HMM, the hidden process and the observable process, are assumed to be a stochastic processes, which are series of events that have an underlying randomness within a probabilistic space. Simply put, the data will have elements of randomness but within a known confine of outputs. For example, daily temperature can be predicted and modelled, but underlying randomness in global weather will never make the recorded data fit 1-to-1 with a model.</p> <p>The hidden process is a markov chain, which is the basis for HMMs. A Markov chain (or Markovian assumption) follows the assumption that the probability of the next state in the sequence is only determined by the one you're in currently, so it's a memory-less system; none of the previous history matters, with a fixed matrix of probabilities that will infer what the next state will be.</p> <p>For those of you who like mathematical equations, see below for the Markov chain rule:</p> <p></p> <p>Back to our checklist briefly now we know what markov chains are:</p> <ol> <li>Can the system be modelled under the Markovian assumption</li> </ol> <p>Most modelling is reductive, we're taking a system with thousands of interacting parts and looking at their probabilistic output. It's therefore not a definitive reflection of reality and our assumptions are the same. Applying this philosophy to the Markovian assumption we do know sleep stage transitions in mammals is determined by the previous as it changes from REM to NREM. But we also recognise it's not a memory-less process, the amount of sleep you've had previously may affect the time periods in each stages. So we'll still plough ahead with the HMM, but we will understand its limitations and use that to help us interpret the output within context.</p> <p>Likewise, the observable (emission in the literature) process is also assumed to be a markov chain, it is assumed that each observed state is only dependent on the state that produced it, independent of any of the prior states, with each hidden state having a probability matrix that represents the likelihood that it will emit any given state.</p> <p>Overall, you can now see that the Hidden Markov Model is quite simple, with the basis being the likelihood of states transitioning into one another and the likelihood they infer an observable state.</p> <p>To sum up the parts you have:</p> <pre><code>    1. The observable process\n            a. A sequence of observable states\n            b. A probability emission matrix (what you train)\n    \n    2. The hidden process\n            a. A state architecture defined by you\n            b. A probability transition matrix (what you train)\n    </code></pre> <p>We'll take these parts and generate representations for our example.</p> <p>For this section, we'll create a scenario to help explain the components.</p> <p>Let's assume students at Imperial are known to have three different states of mood: Tired, Motivated, and Happy. We ask a student to record their mood every day for a year.</p> <p>We'll generate this data and look at it as a Markov chain. For this, we'll be using Numpy, a Python package for creating and manipulating data arrays. You may already be familiar with it, if not, don't fear, everything that is shown will be explained. If curious, you can head to their website for more information -&gt; here</p> In\u00a0[\u00a0]: Copied! <pre># For the next section we'll need to call a function saved elsewhere, to do this we need to add the folder its in to path\nimport sys\nsys.path.append('../src/HMM')\n\n# Now we can import any functions from the misc folder in \nfrom misc import create_chain\n# We also need numpy for the notebook, it is commonaly imported as np to make it shorter\nimport numpy as np\n</pre> # For the next section we'll need to call a function saved elsewhere, to do this we need to add the folder its in to path import sys sys.path.append('../src/HMM')  # Now we can import any functions from the misc folder in  from misc import create_chain # We also need numpy for the notebook, it is commonaly imported as np to make it shorter import numpy as np  In\u00a0[\u00a0]: Copied! <pre># Set the moods we want as a list\nmoods = ['Tired', 'Motivated', 'Happy']\n\n# The length of the markov chain we want to produce \nl = 365\n\n# This function will create a list of different rules from a undiclosed transition matrix\n# We'll work back from the output to the transtion matrix to better understand Markov chains\nmood_chain = create_chain(states = moods, length = l)\n\nmood_chain\n</pre> # Set the moods we want as a list moods = ['Tired', 'Motivated', 'Happy']  # The length of the markov chain we want to produce  l = 365  # This function will create a list of different rules from a undiclosed transition matrix # We'll work back from the output to the transtion matrix to better understand Markov chains mood_chain = create_chain(states = moods, length = l)  mood_chain <p>As you can see, the list is rather long and populated with runs of the states we provided. However, to better understand them, we'll need to change them to numbers, as they're easier to work with.</p> In\u00a0[\u00a0]: Copied! <pre># We'll want to represent our moods as numbers, so first we'll create a dictionary that show us what mood equals what number\nmood_dict = {'Tired' : 0, 'Motivated' : 1, 'Happy' : 2}\n\n# Here we use list comprehension to change each item in the list into its matched value in the dictionary\nmood_array =  np.array([mood_dict[k] for k in mood_chain])\n</pre> # We'll want to represent our moods as numbers, so first we'll create a dictionary that show us what mood equals what number mood_dict = {'Tired' : 0, 'Motivated' : 1, 'Happy' : 2}  # Here we use list comprehension to change each item in the list into its matched value in the dictionary mood_array =  np.array([mood_dict[k] for k in mood_chain]) <p>Now that we have some data, we can calculate the transition matrix for this data. The transition matrix is the likelihood that one state will transition into another; it is represented as a matrix whose shape is the number of states both across and down. So in our case, the rows will be of length 3, and there will be 3 of them. Each row must sum to 1, as it encapsulates all possibilities. See the example below:</p> <p>$ Tmatrix = \\left[\\begin{array}{ccc} 0.1 &amp; 0.5 &amp; 0.4\\\\ 0.6 &amp; 0.2 &amp; 0.2\\\\ 0.3 &amp; 0.4 &amp; 0.3 \\end{array}\\right]$</p> <p>We can calculate the transition matrix for a sequence by counting all the occurrences of each transition.</p> In\u00a0[\u00a0]: Copied! <pre># Don't worry too much about this function unless you want to understand it more closely\n# For now it takes the array and goes through it point by point, finding the count of each transition\n\ndef transition_matrix(transitions):\n    \n    n = 1 + max(transitions) # the number of states, i.e 3\n\n    M = np.zeros((n,n)) # Create an empty matrix of shape 3 x 3\n\n    # Here it loops through the original array and one shifted to the right 1 place\n    # This is give you on every loop a state and its next neighbour, for every occurance the matrix is updated with a count\n    for (i,j) in zip(transitions,transitions[1:]):\n        M[i][j] += 1\n\n    # Now we divide by the total to get the probabilities\n    M = M/M.sum(axis=1, keepdims=True)\n    return M\n</pre> # Don't worry too much about this function unless you want to understand it more closely # For now it takes the array and goes through it point by point, finding the count of each transition  def transition_matrix(transitions):          n = 1 + max(transitions) # the number of states, i.e 3      M = np.zeros((n,n)) # Create an empty matrix of shape 3 x 3      # Here it loops through the original array and one shifted to the right 1 place     # This is give you on every loop a state and its next neighbour, for every occurance the matrix is updated with a count     for (i,j) in zip(transitions,transitions[1:]):         M[i][j] += 1      # Now we divide by the total to get the probabilities     M = M/M.sum(axis=1, keepdims=True)     return M In\u00a0[\u00a0]: Copied! <pre># Call the function and see the output\nt_matrix = transition_matrix(mood_array)\n\nt_matrix\n</pre> # Call the function and see the output t_matrix = transition_matrix(mood_array)  t_matrix <p>We have the transition matrix for our sequence, but is it the transition probability of our system? In terms of time, a year (365 days) may seem like a long time, but in maths it's not much at all, so like how counting the flipping of a coin 100 times won't give you a 50|50 ratio of heads and tails, 365 data points is unlikely to give you the true transition matrix.</p> <p>The \"law of large numbers\" is a principle of probability that states that the frequencies of events with the same likelihood of occurence even out, but only in instances of enough trials. Therefore, the more we increase the length, the more the transition values we get converge on the true values. As you can see, a markov chain and its transition rates are just a reflection of the likelihood that one event will happen after another; the randomness of stoachastic nature occurs as the transition rates are not 1 and therefore not guaranteed.</p> In\u00a0[\u00a0]: Copied! <pre># We'll loop through a few values and you should see the values converge\nfor l in [365, 5000, 10000, 100000]:\n    mood_chain = create_chain(states = moods, start = 'Happy', length = l)\n    mood_array =  np.array([mood_dict[k] for k in mood_chain])\n    t_matrix = transition_matrix(mood_array)\n    print(f'Matrix for length {l}:\\n{t_matrix}\\n')\n</pre> # We'll loop through a few values and you should see the values converge for l in [365, 5000, 10000, 100000]:     mood_chain = create_chain(states = moods, start = 'Happy', length = l)     mood_array =  np.array([mood_dict[k] for k in mood_chain])     t_matrix = transition_matrix(mood_array)     print(f'Matrix for length {l}:\\n{t_matrix}\\n') <p>You should be able to see by eye what the original transition  matrix was:</p> <p>$ MoodMatrix = \\left[\\begin{array}{ccc} 0.70 &amp; 0.05 &amp; 0.25\\\\ 0.35 &amp; 0.50 &amp; 0.15\\\\ 0.05 &amp; 0.30 &amp; 0.65 \\end{array}\\right]$</p> <p>Which can be better read as:</p> <p></p> <p>Or visually as:</p> <p></p> <p>With this transition matrix we could then use it to predict the liklihood of the person being happy 5 days after they were tired, or the liklihood of 3 days of motivation in a row. However, we're not interested in the markov chain but a Hidden Markov Model, but the markov chain is what dictates our hidden states, the transition matrix we created above is the same transition matrix that will be generated in future notebooks for our example (however it won't be worked out as simply as counting the transitions).</p> <p>Extra task:</p> <pre><code>        - Calculate what state is most likely 3 days after being Tired</code></pre> In\u00a0[\u00a0]: Copied! <pre># Code space for extra task\n</pre> # Code space for extra task <p>Now let's take the mood Markov chain and pretend we can't observe them anymore. We've lost the students notes, but we do have someone observing them every day, and they note down for each day if they are mostly procrastinating all day or studying hard. This creates a time-series sequence similar to what we saw before:</p> <p>observable_seq = ['studying', 'studying', 'procratinating', ...]</p> <p>So we don't know what their internal state is, but we're pretty sure the observations are representative of the mood Markov chain. The hidden markov chain (moods) is linked to the observable sequence by an observation likelihood matrix that is similar to the tranisiton matrix, where each state-observable combination has a liklihood of occuring. We call these emissions. We can view it visually below:</p> <p></p> <p>Finally, there is an intial probability distribution matrix, which is the probability that the Markov chain will start in a specific hidden state, but it is not very important for our purpose.</p> <p>The intial probability matrix, the observation matrix, and the transition matrix make up the total parameters of an HMM. Once you have all of them, you can decode any observable sequence from your system of interest.</p> <p>We'll briefly go over two algorithms that will be used later on to train the HMM and decode the sequences. While you don't need to know how they work to use the model, it can help you fully understand the logic around the model and its scores.</p> <p>When we approach a HMM, our first problem is a training or learning problem. Given our observation sequence and our hidden state architecture, we want to know which parameters (the matrices we discussed above) \"best\" explain the relationship between the two. We therefore need to estimate these parameters, which is done by maximising a given score against our training data (observable sequence).</p> <p>To do this, first you make an initial guess of the different matrices, usually randomised. Then the algorithm computes the probability that one hidden state follows another given the guessed transition matrix and the likely observation sequence (the expectation step), which is then compared to the true observation sequence through something called the forward-backward algorithm (the maximisation step). Where a score is given that corresponds to the likelihood of that observable being seen given that hidden state, the cumulative scores per sequence are calculated and used to make an improved guess of the matrices.</p> <p>This process is repeated many times until the score is increased at a minimal rate. We can then say it has converged, and we have approximately found the model that is most likely to produce our given observable sequence.</p> <p>For more general information about the use and understanding of the EM algorithm head here</p> <p>When decoding an observable sequence, you have a computational problem, where the brute force method would be to calculate all possible hidden state paths and then use the forward-backward alogithm to calculate the scores for each, picking the most probable path. However, this is incredibly computationally intensive. See below: starting with a single state, you will see an exponential increase in the number of paths.</p> <p></p> <p>This is where the viterbi algorithm shines, the key to decoding the observation sequence is knowing there is a most probable path. The viterbi algorithm is dynamic, which means it reuses calculated results, saving time and memory. When, at time t, several paths converge, you can discard the less likely paths, and only use the most common ones in the calculation. The number of calculations is reduced to T*N^2, which is a lot smaller than the exponential N^T.</p> <p>Using a road map as an analogy, let's say one is trying to find the shortest path from London to Manchester. If several paths go from London to York and from York to Manchester, it doesn't make sense to join each path from London to York with each path from York to Manchester. The more natural approach is to simply find the shortest path from London to York, and then find the shortest path from York to Manchester. This saves the expense of calculating all the possible path combinations from London to Manchester, which intuitively wouldn't give you a shorter path anyway.</p> <p>For more information about the viterbi algorithm head here</p> <p>Now that we have a decent understanding of Hidden Markov Models, we can head into the data!</p>"},{"location":"1_Understanding_HMMs/#why-hidden-markov-models","title":"Why Hidden Markov Models?\u00b6","text":""},{"location":"1_Understanding_HMMs/#what-are-hidden-markov-models","title":"What are Hidden Markov Models\u00b6","text":""},{"location":"1_Understanding_HMMs/#breaking-down-the-parts","title":"Breaking down the parts\u00b6","text":""},{"location":"1_Understanding_HMMs/#hmm-algorithms","title":"HMM algorithms\u00b6","text":""},{"location":"1_Understanding_HMMs/#expectation-maximisation-algorithm-training","title":"Expectation Maximisation Algorithm: Training\u00b6","text":""},{"location":"1_Understanding_HMMs/#viterbi-algorithm-decoding","title":"Viterbi Algorithm: Decoding\u00b6","text":""},{"location":"2a_Cleaning_your_data/","title":"Applying the HMM to the sleep dataset","text":"<p>Back to the dataset we'll be working with for this tutorial, a Drosophila movement dataset. The data we will be using for this tutorial is real, raw data from the Gilestro lab, where we track and record the movement of fruit flies using machine vision. The tracking is able to discern small movements in the fly several times per second, giving a multitude of variables to work with.</p> <p>We'll be using the Pandas package to import and store our data in the notebooks. Pandas is a widely used tool in data science; it is built on top of Numpy, which we briefly used previously. At the core of Pandas is the DataFrame, a table format you will all be familiar with from spreadsheets. Pandas provides many tools to manipulate the data before you feed it into any analysis or machine learning tool.</p> <p>As with Numpy, everything used here will be explained as we use it, but if you'd like to read more about how to use Pandas, there is a quick tutorial on their website.\u00a0 -&gt; here</p> In\u00a0[1]: Copied! <pre># first we need to import pandas and numpy\n# like numpy it is often imported in a shorthand \nimport pandas as pd\nimport numpy as np\n\n# this pandas setting is to suppress warnings in a later function\npd.set_option(\"future.no_silent_downcasting\", True)\n</pre> # first we need to import pandas and numpy # like numpy it is often imported in a shorthand  import pandas as pd import numpy as np  # this pandas setting is to suppress warnings in a later function pd.set_option(\"future.no_silent_downcasting\", True) <p>Pandas can read many different formats; see here for a detailed list of all file types that can be read and saved to. One of the most common in biology are spreadsheets, or csv files. The training data for this tutorial is saved as a zipped csv file, saved in the data folder called 'training_data.zip'.</p> <p>Copy the path of that file (see below for the exact file structure) and save it as the variable 'path' and as a string. Then load into the notebook using the function pd.read_csv().</p> <pre><code>.\n|\n\u251c\u2500\u2500 data\n|   \u251c\u2500\u2500 example_hmm.pkl\n|   \u251c\u2500\u2500 training_data_metadata.csv\n|   \u2514\u2500\u2500 training_data.zip &lt;----\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 notebooks\n|   \u251c\u2500\u2500 1_Understanding_HMMs.ipynb\n|   \u251c\u2500\u2500 2a_Cleaning_your_data.ipynb\n|   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 src\n</code></pre> In\u00a0[2]: Copied! <pre># If you're using windows you'll need to put an r in front of the string as python doesn't like backslashes\n# .e.g r'C:\\Users\\USER\\Documents\\HMM_tutorial\\data\\training_data.zip'\n\npath = ''\n\n# its common practice to save dataframes as df\ndf = pd.read_csv(path)\n</pre> # If you're using windows you'll need to put an r in front of the string as python doesn't like backslashes # .e.g r'C:\\Users\\USER\\Documents\\HMM_tutorial\\data\\training_data.zip'  path = ''  # its common practice to save dataframes as df df = pd.read_csv(path) In\u00a0[3]: Copied! <pre>df\n</pre> df Out[3]: id t x y w h max_velocity mean_velocity moving micro walk 0 2016-04-04_17-39-22_033aee|01 31140 0.269116 0.069594 0.038829 0.020012 75.662162 25.713480 True False True 1 2016-04-04_17-39-22_033aee|01 31170 0.606590 0.068019 0.048224 0.020609 27.471271 9.145901 True False True 2 2016-04-04_17-39-22_033aee|01 31200 0.398307 0.070464 0.049073 0.020628 19.718721 5.478951 True False True 3 2016-04-04_17-39-22_033aee|01 31230 0.469571 0.066383 0.046558 0.020423 20.224544 7.475374 True False True 4 2016-04-04_17-39-22_033aee|01 31260 0.260085 0.073667 0.047548 0.020133 34.824007 6.163203 True False True ... ... ... ... ... ... ... ... ... ... ... ... 2453010 2016-09-27_10-56-35_053c6b|15 86250 0.776577 0.064865 0.034109 0.022879 0.799611 0.673182 False False False 2453011 2016-09-27_10-56-35_053c6b|15 86280 0.776577 0.064537 0.033866 0.022686 0.774246 0.659115 False False False 2453012 2016-09-27_10-56-35_053c6b|15 86310 0.776577 0.064823 0.035156 0.021957 0.779612 0.679327 False False False 2453013 2016-09-27_10-56-35_053c6b|15 86340 0.776577 0.064693 0.035478 0.022051 0.772465 0.678201 False False False 2453014 2016-09-27_10-56-35_053c6b|15 86370 0.753328 0.064167 0.033769 0.022784 186.156732 13.214985 True False True <p>2453015 rows \u00d7 11 columns</p> <p>The first column is 'id' which contains a unique ID per fly and will allow us to filter and apply methods to just one fly at a time. The next most important variable is 't' or time. As we are working with time series data, we must ensure this is structured properly, i.e., in sequential order and at regular intervals (the later we will go over). The rest are various variables per timestamp; for this tutorial, we'll only be interested in 'moving', 'micro', and 'walk'.</p> <p>Most real datasets will not be perfectly populated, with tracking dropping out over the course of an experiment. In a dataframe or an array where there is data missing for one of columns but not all, the missing data will be represented by a NaN value, which lets methods and functions know there is no data rather than a zero value. However, often analysing packages will throw an error if you feed it NaN values, so it's good practice to check for them first and either remove them or replace them with an approximation.</p> In\u00a0[\u00a0]: Copied! <pre># Lets filter our dataframe for nan values\n# With pandas you can filter the dataframe by the columns\n# To filter or slice the dataframe put some square brackets after the dataframe and inside call the column slice \n# For finding NaN values we have to call a method, for other regular filtering you just use =, &lt;, &gt; and so on\n\ndf[df['x'].isnull()]\n</pre> # Lets filter our dataframe for nan values # With pandas you can filter the dataframe by the columns # To filter or slice the dataframe put some square brackets after the dataframe and inside call the column slice  # For finding NaN values we have to call a method, for other regular filtering you just use =, &lt;, &gt; and so on  df[df['x'].isnull()] In\u00a0[\u00a0]: Copied! <pre># To break down whats happening we can just call whats inside the brackets, you can see that it is an array (or series in pandas terms) with False or True per row.\n# This array then dictates what rows get returned from the whole dataframe, i.e. only the ones that fullfil the argument and are True\n\ndf['x'].isnull()\n</pre> # To break down whats happening we can just call whats inside the brackets, you can see that it is an array (or series in pandas terms) with False or True per row. # This array then dictates what rows get returned from the whole dataframe, i.e. only the ones that fullfil the argument and are True  df['x'].isnull() In\u00a0[\u00a0]: Copied! <pre># However, we are not just looking at one column. \n# Luckily with pandas you can filter by multiple columns, all you need to do is put each filter argument in round brackets and then separate them by an &amp; (\"and\") or | (\"or\") logical operator\n# By calling OR here we get all the examples where X or Y are NaNs\n\ndf[(df['x'].isnull()) | (df['y'].isnull())]\n</pre> # However, we are not just looking at one column.  # Luckily with pandas you can filter by multiple columns, all you need to do is put each filter argument in round brackets and then separate them by an &amp; (\"and\") or | (\"or\") logical operator # By calling OR here we get all the examples where X or Y are NaNs  df[(df['x'].isnull()) | (df['y'].isnull())] In\u00a0[\u00a0]: Copied! <pre># Now we want to remove those rows containing NaN values as they aren't providing any information\n# This time we'll want to call the &amp; operator as we only want rows where both X and Y are not NaNs\n# When filtering for NaNs above we're selecting for them, adding the ~ opertor tells the filter to look for the opposite, so when NaN is True it now becomes False\n# If taking a slice of a dataframe its good practice to make it a copy, otherwise it will throw up warnings\ndf_filtered = df[~(df['x'].isnull()) &amp; ~ (df['y'].isnull())].copy(deep = True)\n\n# the new DataFrame now won't have any rows where 'x' and 'y' have NaN values\ndf_filtered\n</pre> # Now we want to remove those rows containing NaN values as they aren't providing any information # This time we'll want to call the &amp; operator as we only want rows where both X and Y are not NaNs # When filtering for NaNs above we're selecting for them, adding the ~ opertor tells the filter to look for the opposite, so when NaN is True it now becomes False # If taking a slice of a dataframe its good practice to make it a copy, otherwise it will throw up warnings df_filtered = df[~(df['x'].isnull()) &amp; ~ (df['y'].isnull())].copy(deep = True)  # the new DataFrame now won't have any rows where 'x' and 'y' have NaN values df_filtered In\u00a0[\u00a0]: Copied! <pre># To complete \n\ndf =\n</pre> # To complete   df =  <p>It's important with hidden Markov models that any timeseries dataset is complete with no skips due to missing data, as the model will assume the array you're feeding it all has the same time step. One way to do this is to increase the timestep; currently, the dataset has a row for every 30 seconds. However, we know from filtering out the NaN values that we won't have them all. So to achieve this, we will increase the time step to 60. As long as there is at least 1 row out of 2 for the 60, we'll have a perfectly populated dataset.</p> <p>Additionally, doing so will decrease the size of the data, meaning the model will train more quickly. It's always worth trying the model with a few different timesteps to see how this affects the output, and then you can pick the one you think is the most representative and quickest to train.</p> In\u00a0[\u00a0]: Copied! <pre># we'll go through it step by step, before wrapping it all in a function\n# First we'll need the function floor which rounds down numbers\nfrom math import floor\n</pre> # we'll go through it step by step, before wrapping it all in a function # First we'll need the function floor which rounds down numbers from math import floor In\u00a0[\u00a0]: Copied! <pre># first we'll create a new column with the new time step\n# lambda functions are an easy way to apply a function per row with a specific column\n# We then divide the time by our new time and round down. The end result is multiplied by the new time step giving the minimum time as divisable by the time step given\ndf['bin_t'] = df['t'].map(lambda t: 60 * floor(t / 60)) # the t represenst each row value for the column 't'\ndf\n</pre> # first we'll create a new column with the new time step # lambda functions are an easy way to apply a function per row with a specific column # We then divide the time by our new time and round down. The end result is multiplied by the new time step giving the minimum time as divisable by the time step given df['bin_t'] = df['t'].map(lambda t: 60 * floor(t / 60)) # the t represenst each row value for the column 't' df <p>You should see in the column 'bin_t' that rows next to each other now share a time step. Now we have that we'll want to pivot or group by this column so all that have the same time stamp are collected together.</p> In\u00a0[\u00a0]: Copied! <pre># The pandas groupby method does this, all you need to do is call the method with the column you want to pivot by in the brackets\n# Then you can tell it what aggregating function you want to call on the columns of interest\ndf_grouped = df.groupby('bin_t').agg(**{\n            'x' : ('x', 'mean'), # before the brackets is the name of the new column, we'll keep it the same\n            'y' : ('y', 'mean')  # within the brackets is the column you want to use and the function to apply to it. You have 'mean', 'median', 'max'... ect built in, but you can also use your own functions\n})\n\ndf_grouped\n</pre> # The pandas groupby method does this, all you need to do is call the method with the column you want to pivot by in the brackets # Then you can tell it what aggregating function you want to call on the columns of interest df_grouped = df.groupby('bin_t').agg(**{             'x' : ('x', 'mean'), # before the brackets is the name of the new column, we'll keep it the same             'y' : ('y', 'mean')  # within the brackets is the column you want to use and the function to apply to it. You have 'mean', 'median', 'max'... ect built in, but you can also use your own functions })  df_grouped <p>Some of you may have noticed that doing it this way will aggregate our whole dataset and lose information per fly. To keep this information, we can call it a groupby with two levels: the first will be the higher level that the data is grouped by first, and the second will be the one that the functions will be applied to.</p> In\u00a0[\u00a0]: Copied! <pre># We do exactly the same, but instead of just 'bin_t' we have a list with 'id' first\n# Calling it this way on a lot of rows can take a few minutes or more depending on your computer, so don't worry if it takes a while\ndf_grouped = df.groupby(['id', 'bin_t']).agg(**{\n            'x' : ('x', 'mean'),\n            'y' : ('y', 'mean')\n})\n# We need to reset the index as it will have both 'id' and 'bin_t' as the index\ndf_grouped.reset_index(inplace = True)\n# We'll also rename the column 'bin_t' back to 't' for clarity\ndf_grouped.rename(columns = {'bin_t' : 't'}, inplace = True)\ndf_grouped\n</pre> # We do exactly the same, but instead of just 'bin_t' we have a list with 'id' first # Calling it this way on a lot of rows can take a few minutes or more depending on your computer, so don't worry if it takes a while df_grouped = df.groupby(['id', 'bin_t']).agg(**{             'x' : ('x', 'mean'),             'y' : ('y', 'mean') }) # We need to reset the index as it will have both 'id' and 'bin_t' as the index df_grouped.reset_index(inplace = True) # We'll also rename the column 'bin_t' back to 't' for clarity df_grouped.rename(columns = {'bin_t' : 't'}, inplace = True) df_grouped In\u00a0[\u00a0]: Copied! <pre># To complete \n\ndf =\n</pre> # To complete   df =  <p>Another method to fill in the gaps in the data is interpolation. This is where you determine a value at any given timepoint, given the rest of the dataset. If you have just a few points missing, the interpolation results can be quite accurate. Here we'll run through the steps to interpolate your data.</p> In\u00a0[\u00a0]: Copied! <pre># First we can check if we have all the data points for each fly\n# We'll use this method to check\n\ndef check_num_points(data, timestep=60):\n    array_time = max(data['t']) - min(data['t'])\n\n    if (array_time / timestep) + 1 == len(data):\n        return True\n    else:\n        return False\n\n# We need to call the function on each fly individually\n# To do this you cal a groupby with 'id' as the argument\ndf_check = df.groupby('id').apply(check_num_points, include_groups=False) # set include_groups to false when you don't want the grouping column in the analysis\n\n# This gives us a pandas series of True and False for each fly\ndf_check\n</pre> # First we can check if we have all the data points for each fly # We'll use this method to check  def check_num_points(data, timestep=60):     array_time = max(data['t']) - min(data['t'])      if (array_time / timestep) + 1 == len(data):         return True     else:         return False  # We need to call the function on each fly individually # To do this you cal a groupby with 'id' as the argument df_check = df.groupby('id').apply(check_num_points, include_groups=False) # set include_groups to false when you don't want the grouping column in the analysis  # This gives us a pandas series of True and False for each fly df_check In\u00a0[\u00a0]: Copied! <pre># We can count all the True and Falses with the method .value_counts()\ndf_check.value_counts()\n</pre> # We can count all the True and Falses with the method .value_counts() df_check.value_counts() <p>We can see that nearly 50% of our flies are missing some points, so it's best we move ahead with interpolation.</p> In\u00a0[\u00a0]: Copied! <pre># Code space for extra task\n</pre> # Code space for extra task <p>Like when checking for points, we'll need to create a function that we can call to apply the interpolation per fly, as we want it to only use each fly's data. But we'll walk through the steps before creating it. As the data is discrete, we'll be using forward filling interpolation, which propagates the last valid observation to the next, see here for more information.</p> <p>If we were working on continuous data, we could use linear interpolation, which we'll briefly demonstrate at the end with np.interp, a one-dimensional linear interpolator, see here for the documentation from numpy.</p> In\u00a0[\u00a0]: Copied! <pre># for now we'll work with a sub-sample of the main DataFrame so we can check things are working before creating the function\n\n# Task:\n# With the 'id' of '2016-04-04_17-38-06_019aee|20', create a sub DataFrame with just this data\n\nsmall_df =\n</pre> # for now we'll work with a sub-sample of the main DataFrame so we can check things are working before creating the function  # Task: # With the 'id' of '2016-04-04_17-38-06_019aee|20', create a sub DataFrame with just this data  small_df =  In\u00a0[\u00a0]: Copied! <pre># Now we'll want to create a time series that contains all the points we want\n# For this we can use np.arange which creates an array from a given start point to an end point, with regular steps\n# You need to add on you time step to the end as it will only give it to the one step below otherwise\nts_seq = np.arange(min(small_df['t']), max(small_df['t']) + 60, 60)\n\n# You can see it's an array that increase by 60 at each step by checking the difference per point\nnp.all(np.diff(ts_seq) == 60)\n</pre> # Now we'll want to create a time series that contains all the points we want # For this we can use np.arange which creates an array from a given start point to an end point, with regular steps # You need to add on you time step to the end as it will only give it to the one step below otherwise ts_seq = np.arange(min(small_df['t']), max(small_df['t']) + 60, 60)  # You can see it's an array that increase by 60 at each step by checking the difference per point np.all(np.diff(ts_seq) == 60) In\u00a0[\u00a0]: Copied! <pre># Next we'll need to merge this back with the data to create new rows with NaN values which we'll replace\n# To do this we make a pandas series, which is like singular column dataframe, named 't'\n# With both the small dataframe and the series containing 't' we can merge the two together using this column as the key\nts_seq = pd.Series(ts_seq, name = 't')\nsmall_df = small_df.merge(ts_seq, on = 't', how = 'right') # The merge is down to the right as we want the final result to be the length of the new sequence\n\n# Checking for NaN values we can see the new time points are all there\nsmall_df[small_df['moving'].isnull()]\n</pre> # Next we'll need to merge this back with the data to create new rows with NaN values which we'll replace # To do this we make a pandas series, which is like singular column dataframe, named 't' # With both the small dataframe and the series containing 't' we can merge the two together using this column as the key ts_seq = pd.Series(ts_seq, name = 't') small_df = small_df.merge(ts_seq, on = 't', how = 'right') # The merge is down to the right as we want the final result to be the length of the new sequence  # Checking for NaN values we can see the new time points are all there small_df[small_df['moving'].isnull()] In\u00a0[\u00a0]: Copied! <pre># Now all we need to call is ffill\nsmall_df.ffill(inplace=True)\n</pre> # Now all we need to call is ffill small_df.ffill(inplace=True) In\u00a0[\u00a0]: Copied! <pre># The NaNs have bbeen filled\nsmall_df[small_df['moving'].isnull()]\n</pre> # The NaNs have bbeen filled small_df[small_df['moving'].isnull()] In\u00a0[\u00a0]: Copied! <pre>small_df\n</pre> small_df <p>We can now make a function that will complete this for the whole dataset.</p> In\u00a0[\u00a0]: Copied! <pre># Fill in the missing parts with what we've done above\n\ndef fill_interpolate(data, timestep = 60):\n\n    ts_seq = \n    ts_seq = \n\n    new_df = \n    new_df.ffill(inplace=True)\n\n    return new_df\n</pre> # Fill in the missing parts with what we've done above  def fill_interpolate(data, timestep = 60):      ts_seq =      ts_seq =       new_df =      new_df.ffill(inplace=True)      return new_df In\u00a0[\u00a0]: Copied! <pre># Now call a groupby method, applying the interpolate function\ndf = df.groupby('id').apply(fill_interpolate, include_groups=False)\ndf.reset_index(level = 0, inplace = True)\n</pre> # Now call a groupby method, applying the interpolate function df = df.groupby('id').apply(fill_interpolate, include_groups=False) df.reset_index(level = 0, inplace = True) In\u00a0[\u00a0]: Copied! <pre># Lets use the check function to see if its worked\ndf_check = df.groupby('id').apply(check_num_points, include_groups=False)\ndf_check.value_counts()\n</pre> # Lets use the check function to see if its worked df_check = df.groupby('id').apply(check_num_points, include_groups=False) df_check.value_counts() In\u00a0[\u00a0]: Copied! <pre># We'll load in the original dataset to get some continuous data again.\ninterp_df  = pd.read_csv(path)\n\n# We'll check to see if any are missing datapoints\ndf_check = interp_df.groupby('id').apply(check_num_points, include_groups=False)\ndf_check.value_counts()\n</pre> # We'll load in the original dataset to get some continuous data again. interp_df  = pd.read_csv(path)  # We'll check to see if any are missing datapoints df_check = interp_df.groupby('id').apply(check_num_points, include_groups=False) df_check.value_counts() <p>They're all missing points, so we'll use the same specimen as last time.</p> In\u00a0[\u00a0]: Copied! <pre>small_interp = interp_df[interp_df['id'] == '2016-04-04_17-38-06_019aee|20'].copy(deep=True) \nsmall_interp\n</pre> small_interp = interp_df[interp_df['id'] == '2016-04-04_17-38-06_019aee|20'].copy(deep=True)  small_interp In\u00a0[\u00a0]: Copied! <pre># Like before we'll make a new time series of the length and intervals we want\nts_seq = np.arange(min(small_interp['t']), max(small_interp['t']) + 60, 60)\n\n# Call np.interp with the new time series first, the old second, and the corresponding data third\nnew_seq = np.interp(ts_seq, small_interp['t'].to_numpy(), small_interp['x'].to_numpy())\nnew_seq\n</pre> # Like before we'll make a new time series of the length and intervals we want ts_seq = np.arange(min(small_interp['t']), max(small_interp['t']) + 60, 60)  # Call np.interp with the new time series first, the old second, and the corresponding data third new_seq = np.interp(ts_seq, small_interp['t'].to_numpy(), small_interp['x'].to_numpy()) new_seq In\u00a0[\u00a0]: Copied! <pre># Code space for extra task\n</pre> # Code space for extra task  In\u00a0[\u00a0]: Copied! <pre># we can do this for the rest of the columns quickly with a for loop\n# for loops are useful for when you need to do the same thing over and over, with a few things changed\nfor i in ['moving', 'micro', 'walk']:\n    small_df[i] = np.where(small_df[i] == True, 1, 0)\n\n# The columns are now nicely binary\nsmall_df\n</pre> # we can do this for the rest of the columns quickly with a for loop # for loops are useful for when you need to do the same thing over and over, with a few things changed for i in ['moving', 'micro', 'walk']:     small_df[i] = np.where(small_df[i] == True, 1, 0)  # The columns are now nicely binary small_df In\u00a0[\u00a0]: Copied! <pre># Finally we don't know if each flies data has the a good amount of data points for it\n# Flies with a low amount could indicate they died early or the tracking stopped working\n\nlen_check = df.groupby('id').agg(**{\n    'length' : ('t', len)\n})\nlen_check['length'].value_counts()\n\n# You can see most flies have over 9000 data points, but 2 have only 200 odd, we'll want to remove them\n</pre> # Finally we don't know if each flies data has the a good amount of data points for it # Flies with a low amount could indicate they died early or the tracking stopped working  len_check = df.groupby('id').agg(**{     'length' : ('t', len) }) len_check['length'].value_counts()  # You can see most flies have over 9000 data points, but 2 have only 200 odd, we'll want to remove them  In\u00a0[\u00a0]: Copied! <pre># We can find the length of each subset dataframe\n# get the ids of those &gt; 300 data points\n# use this list to filter the whole dataset\n\nlen_df = df.groupby('id').agg(**{\n    'len' : ('t', len)\n})\nfilt_len = len_df[len_df['len'] &lt; 300]\nfilt_list = filt_len.index.tolist()\n\ndf = df[~df['id'].isin(filt_list)]\ndf\n</pre> # We can find the length of each subset dataframe # get the ids of those &gt; 300 data points # use this list to filter the whole dataset  len_df = df.groupby('id').agg(**{     'len' : ('t', len) }) filt_len = len_df[len_df['len'] &lt; 300] filt_list = filt_len.index.tolist()  df = df[~df['id'].isin(filt_list)] df <p>The HMM we'll be using is categorical, which means if we want to use all the information for the 3 columns, we must create a new column that has numbers that represent each variable when they are true. Hmmlearn takes each observable as a number, with the first being 0, the next being 1, and so on. Here we have 3 observables: not moving, micro-moving, and walking, so we would like them to be 0, 1, and 2, respectively.</p> <p>For this we'll be using np.where, a function that takes a logical statemeant to update a column whether it's True or False. See here for more information.</p> In\u00a0[\u00a0]: Copied! <pre># At first we'll look for all rows where the flies aren't moving, if True we label it 0, if not we give it a NaN value for now\ndf['hmm'] = np.where(df['moving'] == 0, 0, np.nan)\n\n# Next we'll look at micro, a fly cannot be both micro moving and walking, they are distinct. So we can just select for True cases.\n# We make the False argument what the column was previously to keep the old category\ndf['hmm'] = np.where(df['micro'] == 1, 1, df['hmm'])\n\n# Now we'll finish with walk, can you complete it?\n# df['hmm'] = np.where()\ndf['hmm'] = np.where(df['walk'] == 1, 2, df['hmm'])\n</pre> # At first we'll look for all rows where the flies aren't moving, if True we label it 0, if not we give it a NaN value for now df['hmm'] = np.where(df['moving'] == 0, 0, np.nan)  # Next we'll look at micro, a fly cannot be both micro moving and walking, they are distinct. So we can just select for True cases. # We make the False argument what the column was previously to keep the old category df['hmm'] = np.where(df['micro'] == 1, 1, df['hmm'])  # Now we'll finish with walk, can you complete it? # df['hmm'] = np.where() df['hmm'] = np.where(df['walk'] == 1, 2, df['hmm']) <p>Before we move on, we need to set the new column to only integers. Pandas has a habit of making data points floats, i.e., a whole number with a decimal point. This will cause problems when training the data, as the model for categorical hmms wants the input to only be integers.</p> In\u00a0[\u00a0]: Copied! <pre># We can set columns types with the .astype() method\ndf = df.astype({'hmm' : 'int'})\n</pre> # We can set columns types with the .astype() method df = df.astype({'hmm' : 'int'}) In\u00a0[\u00a0]: Copied! <pre>df\n</pre> df In\u00a0[\u00a0]: Copied! <pre># Next we'll save our cleaned dataframe as a pickle file \n# Pickles are a popular format to save multiple variable formts\n# Pandas has a built-in function to save the file\ndf.to_pickle('YOUR_PATH/data/cleaned_data.pkl')\n</pre> # Next we'll save our cleaned dataframe as a pickle file  # Pickles are a popular format to save multiple variable formts # Pandas has a built-in function to save the file df.to_pickle('YOUR_PATH/data/cleaned_data.pkl')"},{"location":"2a_Cleaning_your_data/#applying-the-hmm-to-the-sleep-dataset","title":"Applying the HMM to the sleep dataset\u00b6","text":""},{"location":"2a_Cleaning_your_data/#the-data-structure","title":"The Data Structure\u00b6","text":""},{"location":"2a_Cleaning_your_data/#checking-for-missing-data","title":"Checking for missing data\u00b6","text":""},{"location":"2a_Cleaning_your_data/#task","title":"Task:\u00b6","text":"<p>As stated before for this tutorial, we will be focusing on the variables 'moving', 'micro', 'walk'. Now you know how to filter out NaN values apply this to only these columns.</p>"},{"location":"2a_Cleaning_your_data/#extra-task","title":"Extra Task:\u00b6","text":"<ol> <li><p>If you're new to Pandas (or just want some practice), have a play around with other types of filtering (such as df[df['mean_velocity'] &gt; 5]). It's a quick and easy way to filter your data, and if you're doing the same thing repeatedly, you can create a function to do it instantly.</p> </li> <li><p>Rather than filtering out the NaN values, you can replace them with something else. We could know that tracking drops out when the flies are still for a long time, so we could resonably replace all of the NaN's for 'moving', 'micro', and 'walk' with False. This can be done with the .fillna method, see here for how to do it -&gt; fillna.</p> </li> </ol>"},{"location":"2a_Cleaning_your_data/#binning-the-data-to-a-larger-time-step","title":"Binning the data to a larger time step\u00b6","text":""},{"location":"2a_Cleaning_your_data/#task","title":"Task:\u00b6","text":"<p>The same as before, recreate the steps above, but for the columns 'moving', 'micro', 'walk'. Instead of mean, use max, as we care about the most dominant behaviour in that time window, and it will also keep our results as either True or False which are discrete categories.</p>"},{"location":"2a_Cleaning_your_data/#filling-in-the-gaps","title":"Filling in the gaps\u00b6","text":""},{"location":"2a_Cleaning_your_data/#extra-task","title":"Extra Task:\u00b6","text":"<p>Rather than just returning True or False, you can create a function that returns the percentage of points you have for the amount needed. You can then combine this to filter out the flies that have less than 75% of points.</p>"},{"location":"2a_Cleaning_your_data/#linear-interpolation","title":"Linear interpolation\u00b6","text":"<p>For continuous data like the X and Y coordinates, we can use linear interpolation that fills in the data given where it would be placed on a fitted linear line of true data.</p>"},{"location":"2a_Cleaning_your_data/#extra-task","title":"Extra task:\u00b6","text":"<ul> <li>Can you make a function that will use np.interp on the whole interp_df dataset per fly for variables 'x' and 'y'?</li> </ul>"},{"location":"2a_Cleaning_your_data/#coding-our-categories","title":"Coding our categories\u00b6","text":""},{"location":"2a_Cleaning_your_data/#extra-tasks","title":"Extra Tasks\u00b6","text":""},{"location":"2a_Cleaning_your_data/#1-split-the-data-by-male-and-female-into-separate-dataframes","title":"1. Split the data by Male and Female into separate dataframes.\u00b6","text":"<ul> <li>In the same folder (data) as the training data, there is a csv file containing the metadata for the experiment. This includes a column called \"sex,\" which denotes their sex by \"M\" for male and \"F\" for female, along with a column with their ID. Use both of these to filter and split the data into two training datasets.</li> </ul>"},{"location":"2a_Cleaning_your_data/#2-convert-a-continuous-float-column-to-a-discrete-categorical-column","title":"2. Convert a continuous float column to a discrete categorical column\u00b6","text":"<ul> <li>Convert one of the continuous data columns [\"x\", \"y\", \"phi\", \"w\", \"h\"] into discrete categories and use them to train a HMM</li> </ul>"},{"location":"2b_Training/","title":"Training a HMM","text":"<p>Now that our data is cleaned and augmented, ready for training, we can dive into creating our model and training it.</p> <p>Going back to the first tutorial, we will need to think about our observables and how they relate to our hidden states, as well as how our hidden states interact with each other. For the start, we'll be creating the simplest HMM we can for our dataset. We'll take our obsessions\u2014immobile, micro movements, and walking\u2014and assume they are governed by just two hidden states: asleep and awake.</p> <p>We'll create a pictogram of it:</p> <p></p> <p>We can now translate our ideas into a starting transition matrix. When training a HMM, you start with an initialised transition and emission matrix, and then during the training it will make small changes to these parameters within a loop, running the forward-backward algorithm we discussed and outputting a score, which it will aim to improve. Once it has found that its changes are no longer increasing the score at a given rate, it will end the training.</p> <p>So the initial matrix decides the starting point for the training, but it's not very important that it's close to reality, as it will interactively update towards it during training. For now, we'll make a rough estimate of the transition rates.</p> In\u00a0[\u00a0]: Copied! <pre># First we'll create lists with our state and observable names so we know their order\n# Remember previously that we had to convert our observable values to 0, 1, 2 and this is reflected in our list order\n# Likewise with the hidden states, the output of the model when decoding observable runs will have asleep as 0 and awake as 1\nobservables = ['immobile', 'micro', 'walking']\nhidden_states = ['asleep', 'awake']\n</pre> # First we'll create lists with our state and observable names so we know their order # Remember previously that we had to convert our observable values to 0, 1, 2 and this is reflected in our list order # Likewise with the hidden states, the output of the model when decoding observable runs will have asleep as 0 and awake as 1 observables = ['immobile', 'micro', 'walking'] hidden_states = ['asleep', 'awake'] In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\n# Following on from the first part of the tutorial, each row in the matrix is a hidden state and each column is the hidden state it is transitioning into\n# Remember that the sum of each row must equal 1 and the shape is the number of states x itself. So 2 x 2\nt_prob = np.array(  [[0.7, 0.3],\n                    [0.2, 0.8]]\n                )\n\n# We now do the same for the emission probabilities\n# Where we give it a zero value it tells the model that these emissions can't be attributed to a hidden state. So here the asleep state can only have immobile as its emission\nem_prob =  np.array([[1.0, 0.0, 0.0],\n                    [0.3, 0.3, 0.4],])\n</pre> import numpy as np  # Following on from the first part of the tutorial, each row in the matrix is a hidden state and each column is the hidden state it is transitioning into # Remember that the sum of each row must equal 1 and the shape is the number of states x itself. So 2 x 2 t_prob = np.array(  [[0.7, 0.3],                     [0.2, 0.8]]                 )  # We now do the same for the emission probabilities # Where we give it a zero value it tells the model that these emissions can't be attributed to a hidden state. So here the asleep state can only have immobile as its emission em_prob =  np.array([[1.0, 0.0, 0.0],                     [0.3, 0.3, 0.4],]) In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\n# Load the cleaned data from the previous notebook \ndf = pd.read_pickle('Users path to cleaned data pickle')\n</pre> import pandas as pd  # Load the cleaned data from the previous notebook  df = pd.read_pickle('Users path to cleaned data pickle') In\u00a0[\u00a0]: Copied! <pre>df\n</pre> df In\u00a0[\u00a0]: Copied! <pre># First we want to change our data format from pandas to numpy\n# Once again call groupby with the 'hmm' column selected and this time apply a numpy array\n\nar_data = df.groupby('id')['hmm'].apply(np.array)\n</pre> # First we want to change our data format from pandas to numpy # Once again call groupby with the 'hmm' column selected and this time apply a numpy array  ar_data = df.groupby('id')['hmm'].apply(np.array) In\u00a0[\u00a0]: Copied! <pre># This returns a pandas series with each flies data made into a numpy array\n# A pandas series is like a single column from a dataframe\nprint(type(ar_data)) # use type to find what your variable is\nar_data\n</pre> # This returns a pandas series with each flies data made into a numpy array # A pandas series is like a single column from a dataframe print(type(ar_data)) # use type to find what your variable is ar_data In\u00a0[\u00a0]: Copied! <pre># Now we can make it a nested array rather than a series\nar_data = np.array(ar_data)\n</pre> # Now we can make it a nested array rather than a series ar_data = np.array(ar_data) In\u00a0[\u00a0]: Copied! <pre># With numpy you can check the shape of the array with .shape, we can see below that its shape is the number of flies with have in the filtered dataset\nar_data.shape\n</pre> # With numpy you can check the shape of the array with .shape, we can see below that its shape is the number of flies with have in the filtered dataset ar_data.shape <p>When training a model, it's important to split the dataset into a portion you train and a portion you test, as you can't score the model using the data it used to train the model as it will be overfitted to it.</p> <p>A common technique is to use 10% of the dataset for testing post training, with the remaining 90% for the model. Many packages like sklearn (a common machine learning package) have built-in functions, but given their simplicity, we'll code one ourselves.</p> In\u00a0[\u00a0]: Copied! <pre># First we'll need to get the number of flies that equal 10%\ntest_size = 10\ntest_train_split = round(len(ar_data) * (test_size/100))\n\n# Numpy random.permutations will shuffle the order\nrand_runs = np.random.permutation(ar_data)\n\n# Using square brackets after the np.array selects for arrays of a given indices \n# e.g array[0:10] would select the 1st array to the 9th, as 10 is the first array not to be included\n# Not putting a number before or after the colon means either from the first or to the last array respectively \ntrain = rand_runs[test_train_split:]\ntest = rand_runs[:test_train_split]\n</pre> # First we'll need to get the number of flies that equal 10% test_size = 10 test_train_split = round(len(ar_data) * (test_size/100))  # Numpy random.permutations will shuffle the order rand_runs = np.random.permutation(ar_data)  # Using square brackets after the np.array selects for arrays of a given indices  # e.g array[0:10] would select the 1st array to the 9th, as 10 is the first array not to be included # Not putting a number before or after the colon means either from the first or to the last array respectively  train = rand_runs[test_train_split:] test = rand_runs[:test_train_split] In\u00a0[\u00a0]: Copied! <pre># Quickly check the shape to see if it looks right \ntrain.shape\n</pre> # Quickly check the shape to see if it looks right  train.shape In\u00a0[\u00a0]: Copied! <pre>test.shape\n</pre> test.shape <p>The hmmlearn categorical model accept data formatted in a particular way. Firstly, the sequence for each specimen must be an array where each data point is a single array; see the code below.</p> <pre><code>X = [[1], [0], [0], [2], ...]\n</code></pre> <p>Secondly, when working with multiple sequences, they must all be concatenated (joined) into a single array with a secondary array that contains the lengths of each sequence. See below.</p> <pre><code>X1 = [[1], [0], [0], [2]]\nX2 = [[0], [0], [1]]\n\n\nX = np.concatenate([X1, X2])\nlengths = [len(X1), len(X2)]\n</code></pre> <p>Let's get the lengths of each sequence fist. We can use a technique called list comprehension, which is a quicker way to create a list that would normally need a for loop. See here for more information on it.</p> In\u00a0[\u00a0]: Copied! <pre># Using a for loop we can get the length of each array \nfor i in test:\n    print(len(i))\n</pre> # Using a for loop we can get the length of each array  for i in test:     print(len(i)) In\u00a0[\u00a0]: Copied! <pre># Creating a list comprehension puts the for loop into 1 line within a list, with the loop action taking place on the far left\n# The example below takes an array and divides it in 2\ntest_array = np.array([4,4,4,4,4])\n[n/2 for n in test_array]\n</pre> # Creating a list comprehension puts the for loop into 1 line within a list, with the loop action taking place on the far left # The example below takes an array and divides it in 2 test_array = np.array([4,4,4,4,4]) [n/2 for n in test_array] In\u00a0[1]: Copied! <pre># Task:\n# Apply the logic above to create the list of lengths below\n\nlen_seq_train = []\nlen_seq_test = []\n</pre> # Task: # Apply the logic above to create the list of lengths below  len_seq_train = [] len_seq_test = [] <p>The next step is to join all the sequences and reshape the final singular array.</p> In\u00a0[\u00a0]: Copied! <pre># As our data is in a nested array we can call np.concatenate on the whole thing and it will flatten it into a singluar array\nseq_train = np.concatenate(train, axis = 0) # setting the axis as 0 means the action happens across the rows, axis 1 would mean the columns\nseq_test = np.concatenate(test, 0)\n\nseq_train\n</pre> # As our data is in a nested array we can call np.concatenate on the whole thing and it will flatten it into a singluar array seq_train = np.concatenate(train, axis = 0) # setting the axis as 0 means the action happens across the rows, axis 1 would mean the columns seq_test = np.concatenate(test, 0)  seq_train <p>Numpy arrays have a built-in method .reshape() to give it a new shape, you pass the shape inside brackets. For example, a shape of (3, 2) would have the data consist of 3 arrays with two columns each. See below.</p> <pre><code>a = np.array([0, 1, 2, 3, 4, 5])\na.reshape((3, 2))\narray([[0, 1],\n[2, 3],\n[4, 5]])\n</code></pre> <p>Having each data point as its own array is essentially a single column array, so a shape (6, 1) for above would create the right shape, with 6 being the length of the array.</p> <pre><code>a = a.reshape((6, 1))\narray([[0],\n[1],\n[2],\n[3],\n[4],\n[5]])\n</code></pre> <p>We can get the length of the array programatically with len(), but we can also get it by calling -1 instead, which calls for a one shape dimension.</p> <pre><code>a = a.reshape((-1, 1))\narray([[0],\n[1],\n[2],\n[3],\n[4],\n[5]])\n</code></pre> In\u00a0[\u00a0]: Copied! <pre># Use what is shown above to reshape the array\nseq_train =\nseq_test =\n\nseq_train\n</pre> # Use what is shown above to reshape the array seq_train = seq_test =  seq_train <p>In this next section, we'll generate our HMM from the hmmlearn package. The best way to fully understand a model is to look at the package API documentation; most packages will have a website page with them all. Head to this link -&gt; here to see the docstrings for the categorical HMM, the one we'll be using. Some of the docstring of the model has been copied below.</p> <p>Hmmlearn has lots of different HMM that vary with the emission type, discrete (our one) emissions are the most simple.</p> In\u00a0[\u00a0]: Copied! <pre># First we import the model we want from the hmmlearn package\nfrom hmmlearn.hmm import CategoricalHMM\n</pre> # First we import the model we want from the hmmlearn package from hmmlearn.hmm import CategoricalHMM In\u00a0[\u00a0]: Copied! <pre># Simply save the model to a variable to instialise it\nmodel = CategoricalHMM()\n</pre> # Simply save the model to a variable to instialise it model = CategoricalHMM() In\u00a0[\u00a0]: Copied! <pre># However we're going to want to give it some information about the architecture of our HMM\n# With verbose as True we can see the updating scores in real time\nmodel = CategoricalHMM(n_components = len(hidden_states),  n_features = len(observables), params = 'ste', init_params = 's', verbose = True)\n</pre> # However we're going to want to give it some information about the architecture of our HMM # With verbose as True we can see the updating scores in real time model = CategoricalHMM(n_components = len(hidden_states),  n_features = len(observables), params = 'ste', init_params = 's', verbose = True) In\u00a0[\u00a0]: Copied! <pre># We next need to set our starting transition and emission matrices\n# We do this by setting the attributes directly\nmodel.transmat_ = t_prob # .transmat_ is the transtion matrix attribute\nmodel.emissionprob_ = em_prob # ._emissionprob_ is the emission probabilities matrix attribute\n</pre> # We next need to set our starting transition and emission matrices # We do this by setting the attributes directly model.transmat_ = t_prob # .transmat_ is the transtion matrix attribute model.emissionprob_ = em_prob # ._emissionprob_ is the emission probabilities matrix attribute In\u00a0[\u00a0]: Copied! <pre># Call the .fit() method with the sequence and length data to train the model \n# Given the size of the dataset and the default iteration number (10) it'll be very quick\nmodel.fit(seq_train, len_seq_train)\n</pre> # Call the .fit() method with the sequence and length data to train the model  # Given the size of the dataset and the default iteration number (10) it'll be very quick model.fit(seq_train, len_seq_train) In\u00a0[\u00a0]: Copied! <pre># We now have a trained HMM model!\n# We can view the transition matrices as so\nprint(f'Transition matrix: \\n {model.transmat_} \\n')\n\nprint(f'Emission probabilities: \\n {model.emissionprob_}')\n</pre> # We now have a trained HMM model! # We can view the transition matrices as so print(f'Transition matrix: \\n {model.transmat_} \\n')  print(f'Emission probabilities: \\n {model.emissionprob_}') <p>In the misc folder, there is a pre-made function to better view the probability arrays.</p> In\u00a0[\u00a0]: Copied! <pre># As in notebook 1 we'll add the HMM folder to path and import from misc\nimport sys\nsys.path.append('../src/HMM')\nfrom misc import hmm_display\n\nhmm_display(model, hidden_states, observables)\n</pre> # As in notebook 1 we'll add the HMM folder to path and import from misc import sys sys.path.append('../src/HMM') from misc import hmm_display  hmm_display(model, hidden_states, observables) <p>There are two parameters we can change to stop the training when we believe it has reached near its optimal point: tol and number of iterations. Tol is the convergence threshold, the point at which the model stops iterating as the gain in log-liklihood is below the given value. When training with verbose as True we can see the logliklihood printed on the screen on the left, with the right hand column containing the difference to the prior iteration:</p> <pre><code>log liklihood score --&gt; 1 -1518089.61094345 +nan\n                        2 -1093181.25787965 +424908.35306380\n                        3 -1089743.31215639 +3437.94572327\n                        4 -1088738.07582637 +1005.23633002\n                        5 -1088379.61034925 +358.46547712\n                        6 -1088211.51535954 +168.09498971\n                        7 -1088110.95060934 +100.56475020\n                        8 -1088041.78427272 +69.16633661\n                        9 -1087991.29922729 +50.48504543\n                        10 -1087953.64539560 +37.65383169 &lt;-- logliklihood difference\n</code></pre> <p>If the right hand value drops below the tol value, the default is 0.01, the training stops.</p> <p>As we can see in the example above, the logliklihood difference never dropped below 0.01, but the training was still stopped. This is due to a iteration limit, whose default is 10. When training, you need to balance the amount of computer time you can dedicate to training versus the fitness of the model.</p> <p>Have a play around with different iteration numbers and tol to see how it affects the training time and the diminishing logliklihood difference as you increase the iteration number with parameters 'tol' and 'n_iter', respectively.</p> In\u00a0[\u00a0]: Copied! <pre>model = CategoricalHMM(tol = , n_iter = , n_components = len(hidden_states),  n_features = len(observables), params = 'ste', init_params = 'ste', verbose = True)\nmodel.fit(seq_train, len_seq_train)\n</pre> model = CategoricalHMM(tol = , n_iter = , n_components = len(hidden_states),  n_features = len(observables), params = 'ste', init_params = 'ste', verbose = True) model.fit(seq_train, len_seq_train) <p>We have a model, but is it our best model? Remember we created a testing subset that we can generate a score for with this trained model? But what would we score it against?</p> <p>With a fixed starting transition and emission probabilities, we could run the training and choose the model with the best score, thinking we've got the best we could with the data and time we have. However, you can run into a machine learning problem called local minima, where if you start from the same point every time when training, the model will optimise to the best score in that local area. Whereas, in reality, there are much better parameters the model misses out on; see the picture below. If we start at the red mark and iterate over new parameters, it will always fall into the local minima, which has a worse score.</p> <p></p> <p>To get around this, it's best to randomise your starting parameters every time, so your likelihood of finding the global minima is greater. The hmmlearn models have this built-in, where if you don't set the transition probabilities beforehand, it will randomise them.</p> In\u00a0[\u00a0]: Copied! <pre># No longer intialise the model with the transition and probability matrices\nmodel = CategoricalHMM(n_components = len(hidden_states),  n_features = len(observables), params = 'ste', init_params = 'ste', verbose=True)\n</pre> # No longer intialise the model with the transition and probability matrices model = CategoricalHMM(n_components = len(hidden_states),  n_features = len(observables), params = 'ste', init_params = 'ste', verbose=True) In\u00a0[\u00a0]: Copied! <pre># We now want to loop through the model multiple times, each time will have new starting matrices\n# As it loops we'll need to save the best performing model to load and compare, we'll save it as a pickle\n# Save the model to the data file with the name \"2_state_model.pkl\"\nimport pickle\n\nsave_path = 'USERS_PATH/HMM_tutorial/data/2_state_hmm.pkl'\n\niterations = 10 #  loop 10 times\n\nfor i in range(iterations):\n\n    best_score = None\n    best_model = None\n\n    hmm = model.fit(seq_train, len_seq_train)\n    print(\"True Convergence:\" + str(hmm.monitor_.history[-1] - hmm.monitor_.history[-2] &lt; hmm.monitor_.tol)) # If it converges rather than max iterations print True\n    print(\"Final log liklihood score:\" + str(hmm.score(seq_train, len_seq_train))) # Print the final log liklihood score\n\n    score = hmm.score(seq_test, len_seq_test) # .score() generates a log probability score for the given observable, if its larger then previous best model it prints the new matrix and saves it\n    \n    # Store the first score as best_score and then compare it the next loop, if it scores better save it as the best model and score\n    if best_score is None or best_score &lt; score:\n        best_score = score\n        best_model = hmm\n        print('New Matrix: \\n')\n        print(f'Transition matrix: \\n {hmm.transmat_} \\n')\n        print(f'Emission probabilities: \\n {hmm.emissionprob_}')\n\n# Finally save the best model and print it's parameters\nwith open(save_path, \"wb\") as file: pickle.dump(best_model, file)\nhmm_display(hmm, hidden_states, observables)\n</pre> # We now want to loop through the model multiple times, each time will have new starting matrices # As it loops we'll need to save the best performing model to load and compare, we'll save it as a pickle # Save the model to the data file with the name \"2_state_model.pkl\" import pickle  save_path = 'USERS_PATH/HMM_tutorial/data/2_state_hmm.pkl'  iterations = 10 #  loop 10 times  for i in range(iterations):      best_score = None     best_model = None      hmm = model.fit(seq_train, len_seq_train)     print(\"True Convergence:\" + str(hmm.monitor_.history[-1] - hmm.monitor_.history[-2] &lt; hmm.monitor_.tol)) # If it converges rather than max iterations print True     print(\"Final log liklihood score:\" + str(hmm.score(seq_train, len_seq_train))) # Print the final log liklihood score      score = hmm.score(seq_test, len_seq_test) # .score() generates a log probability score for the given observable, if its larger then previous best model it prints the new matrix and saves it          # Store the first score as best_score and then compare it the next loop, if it scores better save it as the best model and score     if best_score is None or best_score &lt; score:         best_score = score         best_model = hmm         print('New Matrix: \\n')         print(f'Transition matrix: \\n {hmm.transmat_} \\n')         print(f'Emission probabilities: \\n {hmm.emissionprob_}')  # Finally save the best model and print it's parameters with open(save_path, \"wb\") as file: pickle.dump(best_model, file) hmm_display(hmm, hidden_states, observables) <p>We can now be confident that if we run enough randomised iterations with an appropriate threshold, we will likely end up with the best trained we could get given the dataset. We can now use this model to decode our observable sequence into a hidden state sequence.</p> In\u00a0[\u00a0]: Copied! <pre># We'll decode the first sequence of the test group\nseq = test[0]\nseq = seq.reshape(-1, 1) # It also needs to be reshped for decoding\n\n# Call the .decode() method with the sequence inside the brackets\n# The method returns two parts, the log liklihood for the sequence and the decoded sequence\nlog_prob, decoded_array = model.decode(seq)\nprint(f'Log probability of the state sequence: {log_prob}')\ndecoded_array\n</pre> # We'll decode the first sequence of the test group seq = test[0] seq = seq.reshape(-1, 1) # It also needs to be reshped for decoding  # Call the .decode() method with the sequence inside the brackets # The method returns two parts, the log liklihood for the sequence and the decoded sequence log_prob, decoded_array = model.decode(seq) print(f'Log probability of the state sequence: {log_prob}') decoded_array <p>We have now successfully created a hidden Markov model and decoded our observable data into our theorised sleep states in the final notebook.</p> <p>In the next step, we will look at making a more complex hidden state architecture while retaining the lessons we've learned above.</p> <p>Going back to the first tutorial we will need to think about our observables and how they relate to our hidden states, and also how our hidden states interact with each other. In our mood example we assumed that each state could transition into each other, however biological systems are not always that free, often with certain routes state transitions must make to access others. Our example of sleep could be one such. Study of sleep in mammals has shown us that sleep stages are sequential, starting with REM sleep and transtitioning through progressive deep sleep stages. If we take this as our base we are going to want to restrict our hidden states so the sleep stages are only acessed by a lighter sleep that can transition into a deep sleep.</p> <p>For this tutorial, we will create a four-state hidden model where we have two sleep states and two awake states. Deep sleep, light sleep, quiet awake, active awake. Deep sleep can only be accessed through light sleep, and active awake can only be accessed through quiet awake.</p> <p>It's often easiest to look at these transition maps in pictograms:</p> <p></p> <p>We now need to put the hidden states within the context of the observables. Given that the basic criteria for possible sleep is immobility, we must let the model know that the observables 'micro movement' and 'walking' are off limits for the sleep states. Whereas the active states can have all the observables as emmissions.</p> <p>Here they are added to the above pictogram:</p> <p></p> In\u00a0[\u00a0]: Copied! <pre># As for the 2 sate model we'll create two lists detailing the names of the observables and hidden states\n\nobservables = ['immobile', 'micro', 'walking']\nhidden_states = ['deep sleep', 'light sleep', 'quiet awake', 'active awake']\n</pre> # As for the 2 sate model we'll create two lists detailing the names of the observables and hidden states  observables = ['immobile', 'micro', 'walking'] hidden_states = ['deep sleep', 'light sleep', 'quiet awake', 'active awake'] In\u00a0[\u00a0]: Copied! <pre># As at the beginning we'll make a transition and emission probability matrix\n# Where we give it a zero value it tells the model that these states can't transition into each other\n# Taking the first row to be \"deep sleep\", the first entry is its transition probability into itself, the next is into \"light sleep\", the next is \"quiet awake\", and finally we have its transition into \"active awake\"\n# The next row is for \"light sleep\", but the columns stay the same, so the first entry is the transition probability into \"deep sleep\", the second entry is for itself, and then so on.\n# Remember the sum of each row must equal 1\nt_prob = np.array([[0.6, 0.25, 0.15, 0.0],\n                    [0.25, 0.6, 0.15, 0.0],\n                    [0.0, 0.2, 0.5, 0.3],\n                    [0.0, 0.0, 0.2, 0.8]])\n\n# Now do the same for the emission probabilities given what emissions are allowed in the pictogram\nem_prob =  np.array(\n                    )\n</pre> # As at the beginning we'll make a transition and emission probability matrix # Where we give it a zero value it tells the model that these states can't transition into each other # Taking the first row to be \"deep sleep\", the first entry is its transition probability into itself, the next is into \"light sleep\", the next is \"quiet awake\", and finally we have its transition into \"active awake\" # The next row is for \"light sleep\", but the columns stay the same, so the first entry is the transition probability into \"deep sleep\", the second entry is for itself, and then so on. # Remember the sum of each row must equal 1 t_prob = np.array([[0.6, 0.25, 0.15, 0.0],                     [0.25, 0.6, 0.15, 0.0],                     [0.0, 0.2, 0.5, 0.3],                     [0.0, 0.0, 0.2, 0.8]])  # Now do the same for the emission probabilities given what emissions are allowed in the pictogram em_prob =  np.array(                     ) In\u00a0[\u00a0]: Copied! <pre># We can now run the model\nmodel = CategoricalHMM(n_components = len(hidden_states),  n_features = len(observables), params = 'ste', init_params = 's', verbose = True)\nmodel.transmat_ = t_prob \nmodel.emissionprob_ = em_prob \nmodel.fit(seq_train, len_seq_train)\n</pre> # We can now run the model model = CategoricalHMM(n_components = len(hidden_states),  n_features = len(observables), params = 'ste', init_params = 's', verbose = True) model.transmat_ = t_prob  model.emissionprob_ = em_prob  model.fit(seq_train, len_seq_train) In\u00a0[\u00a0]: Copied! <pre># As we hoped setting the transition to 0 means it isn't updated at all \nhmm_display(model, hidden_states, observables)\n</pre> # As we hoped setting the transition to 0 means it isn't updated at all  hmm_display(model, hidden_states, observables) <p>However, some of you may have noticed a problem if we take the next step of randomising the transition and emission matrices before training: we can't dictate to it to keep some as 0. We'll therefore have to forgo the built-in hmmlearn randomising system and come up with a way to do it ourselves.</p> In\u00a0[\u00a0]: Copied! <pre># A way to achieve certain values as random and others to stay as 0 is to have a place holder for numbers you wish to be randomised\n# For ours we'll have all numbers we want randomised to be a string called 'rand'\nt_prob_o = np.array([['rand', 'rand', 'rand', 0.0],\n                    ['rand', 'rand', 'rand', 0.0],\n                    [0.0, 'rand', 'rand', 'rand'],\n                    [0.0, 0.0, 'rand', 'rand']])\n\n\nem_prob_o =  np.array([[1.0, 0.0, 0.0],\n                    [1.0, 0.0, 0.0],\n                    ['rand', 'rand', 'rand'],\n                    ['rand', 'rand', 'rand']])\n</pre> # A way to achieve certain values as random and others to stay as 0 is to have a place holder for numbers you wish to be randomised # For ours we'll have all numbers we want randomised to be a string called 'rand' t_prob_o = np.array([['rand', 'rand', 'rand', 0.0],                     ['rand', 'rand', 'rand', 0.0],                     [0.0, 'rand', 'rand', 'rand'],                     [0.0, 0.0, 'rand', 'rand']])   em_prob_o =  np.array([[1.0, 0.0, 0.0],                     [1.0, 0.0, 0.0],                     ['rand', 'rand', 'rand'],                     ['rand', 'rand', 'rand']]) In\u00a0[\u00a0]: Copied! <pre># numpy has a randomiser function np.random.random that generates a random number between 0 and 1\n# list comprehension comes in useful here too\n# As we have a nested array we call a list comprehension within another replacing 'rand' with a random number\nt_prob = np.array([[np.random.random() if y == 'rand' else y for y in x] for x in t_prob_o], dtype = np.float64) # we need to dictate the value type in this as a float\n\n# next we need to make the row sum to 1, we do this by getting the total and finding the fraction for each\nt_prob = np.array([[y / sum(x) for y in x] for x in t_prob], dtype = np.float64)\nt_prob\n\n# Try running this cell multiple times to see the values randomise\n</pre> # numpy has a randomiser function np.random.random that generates a random number between 0 and 1 # list comprehension comes in useful here too # As we have a nested array we call a list comprehension within another replacing 'rand' with a random number t_prob = np.array([[np.random.random() if y == 'rand' else y for y in x] for x in t_prob_o], dtype = np.float64) # we need to dictate the value type in this as a float  # next we need to make the row sum to 1, we do this by getting the total and finding the fraction for each t_prob = np.array([[y / sum(x) for y in x] for x in t_prob], dtype = np.float64) t_prob  # Try running this cell multiple times to see the values randomise <p>Task: Now try the same with the emission probabilities</p> In\u00a0[\u00a0]: Copied! <pre>em_prob = \nem_prob =\n</pre> em_prob =  em_prob =  In\u00a0[\u00a0]: Copied! <pre># Complete here\n</pre> # Complete here"},{"location":"2b_Training/#training-a-hmm","title":"Training a HMM\u00b6","text":""},{"location":"2b_Training/#creating-the-model-architecture","title":"Creating the model architecture\u00b6","text":""},{"location":"2b_Training/#preparing-the-data-for-the-model","title":"Preparing the data for the model\u00b6","text":""},{"location":"2b_Training/#splitting-the-data-into-testtrain","title":"Splitting the data into test/train\u00b6","text":""},{"location":"2b_Training/#shaping-the-data-for-the-model","title":"Shaping the data for the model\u00b6","text":""},{"location":"2b_Training/#inialising-the-model","title":"Inialising the model\u00b6","text":""},{"location":"2b_Training/#hmmlearn-docstring","title":"hmmlearn docstring\u00b6","text":"<p>CategoricalHMM(n_components=1, startprob_prior=1.0, transmat_prior=1.0, emissionprob_prior=1.0, n_features=None, algorithm='viterbi', random_state=None, n_iter=10, tol=0.01, verbose=False, params='ste', init_params='ste', implementation='log'):</p> <p>Hidden Markov Model with categorical (discrete) emissions.</p> <p>Variables : n_components (int) \u2013 Number of hidden states.</p> <p>n_features (int) \u2013 Number of possible symbols emitted by the model (in the samples).</p> <p>params (string, optional) \u2013 The parameters that get updated during (params) or initialized before (init_params) the training. Can contain any combination of \u2018s\u2019 for startprob, \u2018t\u2019 for transmat, and \u2018e\u2019 for emissionprob. Defaults to all parameters.</p> <p>init_params (string, optional) \u2013 The parameters that get updated during (params) or initialized before (init_params) the training. Can contain any combination of \u2018s\u2019 for startprob, \u2018t\u2019 for transmat, and \u2018e\u2019 for emissionprob. Defaults to all parameters.</p> <p>verbose (bool, optional) \u2013 Whether per-iteration convergence reports are printed.</p>"},{"location":"2b_Training/#training-the-best-model","title":"Training the best model\u00b6","text":""},{"location":"2b_Training/#decoding-your-observables","title":"Decoding your observables\u00b6","text":""},{"location":"2b_Training/#models-with-limited-transitions","title":"Models with limited transitions\u00b6","text":""},{"location":"2b_Training/#randomising-matrices","title":"Randomising matrices\u00b6","text":""},{"location":"2b_Training/#final-task","title":"Final Task:\u00b6","text":"<p>Take everything we've now learned and train the four-state model set out above. Have it loop through multiple times, randomising the transition and emission matrices. Play around with both the external and internal iteration numbers, as well as tol, to see what is the best score you can get for the model. Save the best model as '4_state_model.pkl' in the data folder.</p>"},{"location":"2c_Validating/","title":"Scoring Models","text":"<p>In the last notebook, we learned how to select the best model given a set model architecture, but often you may have multiple versions of the architecture in mind that you would like to test. A common method is to use probabilistic statistical measures that attempt to quantify both the model performance on the training dataset and the complexity of the model. The scores often used are Akaike and Bayseian Information Criterion (AIC &amp; BIC respectively). Both evaluate the model's fit on the training data, adding penalties for more complex models as these tend to overfit to the dataset. This means the scores will reflect the model that best generalises to the dataset.</p> <p>With criterion values, the lower the score, the better, and it is relative, which means it can only be compared with other models trained on the same dataset and in the same way. Both AIC and BIC evaluate in very similar ways with minor differences in their formulas, so the results should often be very similar.</p> <p>It is important to understand the limitations of probabilistic scores for models when viewing the results. Both AIC and BIC will by design prioritise the simplest model that best fits the dataset, it will have no knowledge of the uncertainty of the model or any biological relevance. It is therefore up to you to decide, using both the scores and your prior knowledge of the system.</p> <p>For this example we will load in the whole dataset and train a fully open HMM (as in everything can transition into each other and all states can emit all observables) for the sake of ease</p> In\u00a0[\u00a0]: Copied! <pre># We'll load in the pacakges we need\nimport pandas as pd\nimport numpy as np\nimport pickle \nfrom hmmlearn.hmm import CategoricalHMM\n\n# Load in your cleaned dataset\ndf = pd.read_pickle('/USERS_PATH/ReCoDE-HMMs-for-the-discovery-of-behavioural-states/admin/cleaned_data.pkl')\n\n# List the observables\nobservables = ['immobile', 'micro', 'walking']\n</pre> # We'll load in the pacakges we need import pandas as pd import numpy as np import pickle  from hmmlearn.hmm import CategoricalHMM  # Load in your cleaned dataset df = pd.read_pickle('/USERS_PATH/ReCoDE-HMMs-for-the-discovery-of-behavioural-states/admin/cleaned_data.pkl')  # List the observables observables = ['immobile', 'micro', 'walking']  In\u00a0[\u00a0]: Copied! <pre># Transform all the data to the right shape\n\nar_data = df.groupby('id')['hmm'].apply(np.array)\nar_data = np.array(ar_data)\n\nlen_seq_all = [len(ar) for ar in ar_data]\n\nseq_all = np.concatenate(ar_data, axis = 0) \nseq_all = seq_all.reshape(-1, 1)\n</pre> # Transform all the data to the right shape  ar_data = df.groupby('id')['hmm'].apply(np.array) ar_data = np.array(ar_data)  len_seq_all = [len(ar) for ar in ar_data]  seq_all = np.concatenate(ar_data, axis = 0)  seq_all = seq_all.reshape(-1, 1)  In\u00a0[\u00a0]: Copied! <pre>seq = ar_data[0]\nseq = seq.reshape(-1, 1) # It also needs to be reshped for decoding\n\n# Call the .decode() method with the sequence inside the brackets\n# The method returns two parts, the log liklihood for the sequence and the decoded sequence\nlog_prob, decoded_array = model.decode(seq)\n</pre> seq = ar_data[0] seq = seq.reshape(-1, 1) # It also needs to be reshped for decoding  # Call the .decode() method with the sequence inside the brackets # The method returns two parts, the log liklihood for the sequence and the decoded sequence log_prob, decoded_array = model.decode(seq) <p>All hmmlearn hidden markov models have a built in method that will give you AIC, BIC scores, as well as the .score() method we've used previously that gives the log likelihood. We'll run through briefly how to get these scores before creating models with varying numbers of hidden states.</p> In\u00a0[\u00a0]: Copied! <pre># Lets load in the 2 state and 4 state model you trained previously into list called models\nmodels = []\n# replace the paths below with the ones in your repo\nmodels.append(pickle.load(open('.../ReCoDE-HMMs-for-the-discovery-of-behavioural-states/data/2_state_model.pkl', \"rb\")))\nmodels.append(pickle.load(open('.../ReCoDE-HMMs-for-the-discovery-of-behavioural-states/data/4_state_model.pkl', \"rb\")))\n\n# We'll create some empty lists to append the scores\naic = []\nbic = []\nlls = []\n\nfor hmm in models:\n    aic.append(hmm.aic(seq_all, len_seq_all)) # get the AIC score with .aic()\n    bic.append(hmm.bic(seq_all, len_seq_all)) #  get the BIC score with .bic()\n    lls.append(hmm.score(seq_all, len_seq_all))# get the logliklihood will .score()\n</pre> # Lets load in the 2 state and 4 state model you trained previously into list called models models = [] # replace the paths below with the ones in your repo models.append(pickle.load(open('.../ReCoDE-HMMs-for-the-discovery-of-behavioural-states/data/2_state_model.pkl', \"rb\"))) models.append(pickle.load(open('.../ReCoDE-HMMs-for-the-discovery-of-behavioural-states/data/4_state_model.pkl', \"rb\")))  # We'll create some empty lists to append the scores aic = [] bic = [] lls = []  for hmm in models:     aic.append(hmm.aic(seq_all, len_seq_all)) # get the AIC score with .aic()     bic.append(hmm.bic(seq_all, len_seq_all)) #  get the BIC score with .bic()     lls.append(hmm.score(seq_all, len_seq_all))# get the logliklihood will .score() <p>We can now use Matplotlib to plot the data. If you've used Python before, you've probably seen or used Matplotlib before, if not, it's a library for visualising data in Python. Click the embedded link for more information.</p> In\u00a0[\u00a0]: Copied! <pre># This is the way to load matplotlib \nimport matplotlib.pyplot as plt\n\n# Labels for the x-axis\nmodel_names = ['2 states', '4 states']\n\n# Create the plot\nfig, ax = plt.subplots()\n\n# Plot AIC and BIC on the first y-axis\nln1 = ax.plot(model_names, aic, label=\"AIC\", color=\"blue\", marker=\"s\")\nln2 = ax.plot(model_names, bic, label=\"BIC\", color=\"green\", marker=\"D\")\n# Create a second y-axis for logliklihood as its scores differently\nax2 = ax.twinx()\nln3 = ax2.plot(model_names, lls, label=\"LL\", color=\"orange\", marker=\"o\")\n\n# Joins the legends and sets the labels\nax.legend(handles=ax.lines + ax2.lines)\nax.set_title(\"Using AIC/BIC for Model Selection\")\nax.set_ylabel(\"Criterion Value (lower is better)\")\nax2.set_ylabel(\"LL (higher is better)\")\nax.set_xlabel(\"HMM type\")\nfig.tight_layout()\n\n# Pring the plot to screen\nplt.show()\n</pre> # This is the way to load matplotlib  import matplotlib.pyplot as plt  # Labels for the x-axis model_names = ['2 states', '4 states']  # Create the plot fig, ax = plt.subplots()  # Plot AIC and BIC on the first y-axis ln1 = ax.plot(model_names, aic, label=\"AIC\", color=\"blue\", marker=\"s\") ln2 = ax.plot(model_names, bic, label=\"BIC\", color=\"green\", marker=\"D\") # Create a second y-axis for logliklihood as its scores differently ax2 = ax.twinx() ln3 = ax2.plot(model_names, lls, label=\"LL\", color=\"orange\", marker=\"o\")  # Joins the legends and sets the labels ax.legend(handles=ax.lines + ax2.lines) ax.set_title(\"Using AIC/BIC for Model Selection\") ax.set_ylabel(\"Criterion Value (lower is better)\") ax2.set_ylabel(\"LL (higher is better)\") ax.set_xlabel(\"HMM type\") fig.tight_layout()  # Pring the plot to screen plt.show() <p>The lower the AIC and BIC, the better, and the higher the likelihood, the better. From this, we can see that despite the additional complexity of the four-state model, it performs better on the dataset in all scores.</p> In\u00a0[\u00a0]: Copied! <pre># Finish the loop below using what you created in the previous notebook\n# hint: what we did for storing the best model\n\naic = []\nbic = []\nlls = []\n\n# Create models of size 2, 4, 6, 8\nn_states = [2, 4, 6, 8]\n\nfor n in n_states:\n</pre> # Finish the loop below using what you created in the previous notebook # hint: what we did for storing the best model  aic = [] bic = [] lls = []  # Create models of size 2, 4, 6, 8 n_states = [2, 4, 6, 8]  for n in n_states: In\u00a0[\u00a0]: Copied! <pre># Write the code to plot the scores here\n</pre> # Write the code to plot the scores here <p>Head to notebook_answers for an example of what the graph should look like, as well as some commentary on how to interpret it.</p> <pre><code>|\n\u251c\u2500\u2500 data\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 notebooks\n\u251c\u2500\u2500 src\n|   \u251c\u2500\u2500 notebook_answers.ipynb &lt;-----\n|   \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"2c_Validating/#scoring-models","title":"Scoring Models\u00b6","text":""},{"location":"2c_Validating/#task","title":"Task\u00b6","text":"<p>Create a loop below that evaluates the models, with varying amounts of hidden states</p>"},{"location":"2c_Validating/#extra-task","title":"Extra task\u00b6","text":"<p>Try training models with varying numbers of hidden states that are true to the biology (i.e., the sleep states only emit as immobile and sleep stages are sequential).</p> <p>Compare the scores for each model.</p>"},{"location":"3_Visualising_the_results/","title":"Visualising the model output","text":"<p>For this section we'll need to import some plotting functions from elsewhere in the repository.</p> In\u00a0[2]: Copied! <pre>import sys\n# The plotting functions are in the HMM folder\nsys.path.append('../src/HMM')\n</pre> import sys # The plotting functions are in the HMM folder sys.path.append('../src/HMM') In\u00a0[3]: Copied! <pre># We'll look at four plots in this notebook\nfrom hmm_plot_functions import plot_hmm_overtime, plot_hmm_quantify, plot_hmm_quantify_length, plot_hmm_raw\n\nfrom misc import hmm_display\n</pre> # We'll look at four plots in this notebook from hmm_plot_functions import plot_hmm_overtime, plot_hmm_quantify, plot_hmm_quantify_length, plot_hmm_raw  from misc import hmm_display <p>For this notebook we'll use a pre-trained HMM, as the dataset we used earlier was deliberately reduced in size to meet the github size limit and to minimise the time spent training. Due to this, there's a good chance the trained model might HAVE some odd quirks, so to be safe we'll use one trained on the whole dataste.</p> In\u00a0[4]: Copied! <pre># we'll need to load in the HMM from the data folder, it should be called example_hmm.pkl\nimport pickle\n\npath  = '.../ReCoDE-HMMs-for-the-discovery-of-behavioural-states/data/example_hmm.pkl'\n\nh = pickle.load(open(path, \"rb\"))\n</pre> # we'll need to load in the HMM from the data folder, it should be called example_hmm.pkl import pickle  path  = '.../ReCoDE-HMMs-for-the-discovery-of-behavioural-states/data/example_hmm.pkl'  h = pickle.load(open(path, \"rb\")) <p>We'll also need to set the labels for the decoded states and the colours we want to represent it.</p> In\u00a0[5]: Copied! <pre>hmm_labels = ['deep sleep', 'light sleep', 'quiet awake', 'active awake']\nemission_labels = observables = ['immobile', 'micro', 'walking']\ncolours = ['darkblue', 'lightblue', 'orange', 'red']\n\n# We can also view the pre-trained model\nhmm_display(h, hmm_labels, emission_labels)\n</pre> hmm_labels = ['deep sleep', 'light sleep', 'quiet awake', 'active awake'] emission_labels = observables = ['immobile', 'micro', 'walking'] colours = ['darkblue', 'lightblue', 'orange', 'red']  # We can also view the pre-trained model hmm_display(h, hmm_labels, emission_labels) <pre>Starting probabilty table: \n|    |   deep sleep |   light sleep |   quiet awake |   active awake |\n|----|--------------|---------------|---------------|----------------|\n|  0 |    0.0165855 |   6.08132e-05 |      0.160616 |       0.822738 |\n\nTransition probabilty table: \n|              |   deep sleep |   light sleep |   quiet awake |   active awake |\n|--------------|--------------|---------------|---------------|----------------|\n| deep sleep   |     0.834628 |     0.0101301 |     0.155242  |      0         |\n| light sleep  |     0.112523 |     0.672489  |     0.214988  |      0         |\n| quiet awake  |     0        |     0.25632   |     0.731348  |      0.0123321 |\n| active awake |     0        |     0         |     0.0206226 |      0.979377  |\n\nEmission probabilty table: \n|              |   immobile |     micro |   walking |\n|--------------|------------|-----------|-----------|\n| deep sleep   |  1         | 0         |  0        |\n| light sleep  |  1         | 0         |  0        |\n| quiet awake  |  0.16186   | 0.418199  |  0.419942 |\n| active awake |  0.0115003 | 0.0185279 |  0.969972 |\n\n</pre> In\u00a0[6]: Copied! <pre># Now we can load in the data we used to train the model earlier, i.e. 'cleaned_data.pkl'\nimport pandas as pd \n\ndf = pd.read_pickle('')\n</pre> # Now we can load in the data we used to train the model earlier, i.e. 'cleaned_data.pkl' import pandas as pd   df = pd.read_pickle('') <p>For plotting the data we'll be using matplotlib and seaborn to create the plots. Matplotlib was introduced in the last notebook and seaborn acts ontop of matplotlib and alongside pandas, allowing users to quickly generate plots from a dataframe with just 1 line of code. Head to the websites linked to read more about their uses. For now we'll just be calling functions that utilise them in the backend, so they're won't be any explination to how the plots were generated.</p> <p>One of the first things you want to look at with any time series analysis is how your output changes overtime. Below we'll generate a graph that plots the prevalence of each state over a 24 hour window.</p> In\u00a0[7]: Copied! <pre># The function below takes the data and the column to decode as the first 2 arguments\n# wrapped takes the decoded data and transforms it to only be over 24 hours, rather than several days\n# tbin is the time difference of the data that was used to train the model (here its 60). Make sure you remember the time difference when training future models as it can affect the output.\n# avg_window is the amount of values that are included in a moving average window that smoothes the data for plotting\nplot_hmm_overtime(data = df, variable = 'hmm', hmm = h, labels = hmm_labels, colours = colours, wrapped = True, tbin = 60, avg_window = 30)\n</pre> # The function below takes the data and the column to decode as the first 2 arguments # wrapped takes the decoded data and transforms it to only be over 24 hours, rather than several days # tbin is the time difference of the data that was used to train the model (here its 60). Make sure you remember the time difference when training future models as it can affect the output. # avg_window is the amount of values that are included in a moving average window that smoothes the data for plotting plot_hmm_overtime(data = df, variable = 'hmm', hmm = h, labels = hmm_labels, colours = colours, wrapped = True, tbin = 60, avg_window = 30) <p>Each plotted line for each state has the 95% confidence intervals shown in a semi-transparent colour around the line. Those sections of each line that have no overlapping confidence intervals we can be fairly sure that they are statistically significant from each other (although we would need to back this up with tests, but at a quick glance its good)</p> <p>Comparing this plot to a regular sleep plot that uses the 5 minute rule to calculate sleep (the Drosophila standard, &gt; 5 minutes of immobility equates sleep) we can see that the peaks in deep sleep follows directly the sleep pattern:</p> <p></p> <p>We can therefore be fairly happy that our HMM is replicating real life (or at least what we think is a correct interpretation of real life).</p> <p>Now that we can see how the states change in prevelance overtime we'll want to quantify the values to see their proportions.</p> <p>With quantification plots bar charts are often not the best way to go as they lack description of the underlying data, just showing the mean or median. It is common now to plot all the data points, with an overlay of a boxplot or violin plot. Seaborn has built in plotting functions to plot boxplots and swarm plots together.</p> In\u00a0[8]: Copied! <pre># We'll call the same arguments as before\n# This plot will find the proportion of time each fly spends in each state\n# The writtern values is the mean, whereas the line in the box is the median\nplot_hmm_quantify(data = df, hmm = h, variable = 'hmm', labels = hmm_labels, colours = colours, tbin = 60)\n</pre> # We'll call the same arguments as before # This plot will find the proportion of time each fly spends in each state # The writtern values is the mean, whereas the line in the box is the median plot_hmm_quantify(data = df, hmm = h, variable = 'hmm', labels = hmm_labels, colours = colours, tbin = 60) <p>We can see that both light sleep and quiet awake are the most prevelant states, which if we look back at the overtime plot makes sense as they are consistently present throughout the day and night with deep sleep and active awake fluctuating throughout the day.</p> <p>We also want to look at the rough length of each state to gain an idea if the states are being transitioned into a lot or just have long instances a few amount of times.</p> In\u00a0[9]: Copied! <pre># Same again\nplot_hmm_quantify_length(data = df, hmm = h, variable = 'hmm', labels = hmm_labels, colours = colours, tbin = 60)\n</pre> # Same again plot_hmm_quantify_length(data = df, hmm = h, variable = 'hmm', labels = hmm_labels, colours = colours, tbin = 60) <pre>/home/lab/Desktop/ReCoDE-HMMs-for-the-discovery-of-behavioural-states/.venv/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 52.6% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/home/lab/Desktop/ReCoDE-HMMs-for-the-discovery-of-behavioural-states/.venv/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 25.2% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n</pre> <p>From this plot we can tell that although both light sleep and quiet awake are the most prevelant, they are both short bouts, indicating they are transitioned into frequently, but for short amount of time. Addtionally, this HMM would predict sleep to be shorter than the tradtional 5 minute rule, an assumption that would need to tested by an experiment.</p> <p>Both deep sleep and active awake are much longer, running for long periods without interuption, which is something you'd expect.</p> <p>Sometimes its good to visualise the raw decoded data to do some exploratory analysis. Looking at the raw data can give you an idea how the states interact with each other, then with any assumptions you make you can devise new analysis techniques to prov your assumptions.</p> In\u00a0[10]: Copied! <pre># Once again the same...\nplot_hmm_raw(data = df, hmm = h, variable = 'hmm', colours = colours, tbin = 60)\n</pre> # Once again the same... plot_hmm_raw(data = df, hmm = h, variable = 'hmm', colours = colours, tbin = 60) <p>With the above plots you can get a general idea about how the model maps to the real dataset and better understand how the transition rates translate in reality. Using these insights you can then create an assay that will test the models output. For example, to test the predicted sleep states I random delivered attractive odours to individual flies and observed if the woke up after (started moving), this data was then decoded and the response mapped to what the state was previously. See the plot below.</p> <p></p> <p>(a) The % of time spent in each state over 24 hours (b) The response rate to a puff of 10% Apple cideo vinegar per state in a 4-state model. In grey is the spontaneuous movement, where a fake puff is recored and then movement post fake puff is recorded.</p> <p>I hope now you are familiar with HMM's, their use, how to create your own, and finally how to understand what's produced and how it maps to reality.</p>"},{"location":"3_Visualising_the_results/#visualising-the-model-output","title":"Visualising the model output\u00b6","text":""},{"location":"3_Visualising_the_results/#plots-overtime","title":"Plots Overtime\u00b6","text":""},{"location":"3_Visualising_the_results/#quantifying-plots","title":"Quantifying plots\u00b6","text":""},{"location":"3_Visualising_the_results/#quantifying-length","title":"Quantifying length\u00b6","text":""},{"location":"3_Visualising_the_results/#plotting-raw-decoded-data","title":"Plotting raw decoded data\u00b6","text":""}]}